{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6ktSXdQVb9Li"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L7Inb1JbiDy"
      },
      "source": [
        "## 3.1 Автоматическое дифференцирование в `torch`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsZ69HZ0EsI7"
      },
      "source": [
        "3.1.1 Воспользовавшись классами `Neuron` и `SquaredLoss` из задачи 2.4.1 и автоматическим дифференцированием, которое предоставляет `torch`, решить задачу регрессии. Для оптимизации использовать стохастический градиетный спуск."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "zynPAaOrRKTm"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
        "X = torch.from_numpy(X).to(dtype=torch.float32)\n",
        "y = torch.from_numpy(y).to(dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    \"\"\"Класс нейрона\"\"\"\n",
        "    def __init__(self, n_inputs):        \n",
        "        #Атрибут weights\n",
        "        self.W = torch.randn(n_inputs)\n",
        "        #Атрибут bias\n",
        "        self.B = torch.randn(1)    \n",
        "  \n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        return torch.matmul(inputs, self.W.T) + self.B\n",
        "      \n",
        "    def backward(self, dvalue):\n",
        "         \n",
        "        self.dweights = dvalue * self.inputs\n",
        "        self.dinput =  dvalue * self.W\n",
        "        self.dbias = dvalue \n",
        "        \n",
        "        # Возвращаем градиент весов и смещения\n",
        "        return self.dweights, self.dbias"
      ],
      "metadata": {
        "id": "yd1oxgbCwpIo"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SquaredLoss:    \n",
        "    \"\"\"Функция потерь\"\"\"\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.y_pred = torch.tensor(y_pred, requires_grad=True)\n",
        "        y_true = torch.tensor(y_true)\n",
        "        z = (self.y_pred - y_true) ** 2\n",
        "        self.z = z\n",
        "        return z\n",
        "\n",
        "    def backward(self):\n",
        "        self.z.backward()\n",
        "        self.dinput = self.y_pred.grad"
      ],
      "metadata": {
        "id": "mtdN9Bjywq_i"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <размерность элемента выборки >\n",
        "n_inputs = 4\n",
        "# скорость обучения\n",
        "learning_rate = 0.01\n",
        "# количество эпох\n",
        "n_epoch = 50\n",
        "#размер пакета\n",
        "batch_size = 10\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = SquaredLoss()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(n_epoch):\n",
        "    sample = torch.randint(0, X.shape[0], size=(batch_size,))\n",
        "    for x_example, y_example in zip(X[sample], y[sample]):\n",
        "        \n",
        "        \n",
        "        # Активация\n",
        "        y_pred = neuron.forward(x_example)\n",
        "        curr_loss = loss.forward(y_pred, y_example)\n",
        "        losses.append(curr_loss)\n",
        "\n",
        "        # Обратное распространение\n",
        "        loss.backward()\n",
        "        dweights, dbias = neuron.backward(loss.dinput)\n",
        "\n",
        "        # Обновление вестов\n",
        "        neuron.W -= learning_rate * dweights\n",
        "        neuron.B -= learning_rate * dbias\n",
        "        print(f\"Epoch {epoch} loss -> {curr_loss[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KW9eBy8ww1x",
        "outputId": "4a3a69ff-bda9-4dc1-8a68-1a08bc878f7e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-97f38f7387e6>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.y_pred = torch.tensor(y_pred, requires_grad=True)\n",
            "<ipython-input-48-97f38f7387e6>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_true = torch.tensor(y_true)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss -> 77867.9921875\n",
            "Epoch 0 loss -> 45462.69921875\n",
            "Epoch 0 loss -> 19075.654296875\n",
            "Epoch 0 loss -> 1203.22021484375\n",
            "Epoch 0 loss -> 11434.77734375\n",
            "Epoch 0 loss -> 46.673587799072266\n",
            "Epoch 0 loss -> 32987.4140625\n",
            "Epoch 0 loss -> 42109.25390625\n",
            "Epoch 0 loss -> 15166.1875\n",
            "Epoch 0 loss -> 2675.65869140625\n",
            "Epoch 1 loss -> 1242.05322265625\n",
            "Epoch 1 loss -> 2897.7158203125\n",
            "Epoch 1 loss -> 3658.207275390625\n",
            "Epoch 1 loss -> 7356.93994140625\n",
            "Epoch 1 loss -> 722.6055908203125\n",
            "Epoch 1 loss -> 314.6275329589844\n",
            "Epoch 1 loss -> 97278.3125\n",
            "Epoch 1 loss -> 1982.1163330078125\n",
            "Epoch 1 loss -> 13138.35546875\n",
            "Epoch 1 loss -> 12109.861328125\n",
            "Epoch 2 loss -> 2762.3076171875\n",
            "Epoch 2 loss -> 5214.115234375\n",
            "Epoch 2 loss -> 0.5925835371017456\n",
            "Epoch 2 loss -> 287.7566223144531\n",
            "Epoch 2 loss -> 5625.26416015625\n",
            "Epoch 2 loss -> 848.1453247070312\n",
            "Epoch 2 loss -> 15843.0595703125\n",
            "Epoch 2 loss -> 899.9755249023438\n",
            "Epoch 2 loss -> 730.2890625\n",
            "Epoch 2 loss -> 340.3201904296875\n",
            "Epoch 3 loss -> 6916.43505859375\n",
            "Epoch 3 loss -> 6820.00634765625\n",
            "Epoch 3 loss -> 0.045155275613069534\n",
            "Epoch 3 loss -> 17808.025390625\n",
            "Epoch 3 loss -> 495.60687255859375\n",
            "Epoch 3 loss -> 1944.572998046875\n",
            "Epoch 3 loss -> 11963.85546875\n",
            "Epoch 3 loss -> 4830.51513671875\n",
            "Epoch 3 loss -> 2872.90576171875\n",
            "Epoch 3 loss -> 1384.5255126953125\n",
            "Epoch 4 loss -> 4923.5009765625\n",
            "Epoch 4 loss -> 5043.27099609375\n",
            "Epoch 4 loss -> 5775.7333984375\n",
            "Epoch 4 loss -> 2908.73046875\n",
            "Epoch 4 loss -> 555.5506591796875\n",
            "Epoch 4 loss -> 2.169266939163208\n",
            "Epoch 4 loss -> 12570.44140625\n",
            "Epoch 4 loss -> 145.186767578125\n",
            "Epoch 4 loss -> 0.19437837600708008\n",
            "Epoch 4 loss -> 379.67327880859375\n",
            "Epoch 5 loss -> 374.83355712890625\n",
            "Epoch 5 loss -> 16.17009735107422\n",
            "Epoch 5 loss -> 607.60791015625\n",
            "Epoch 5 loss -> 77.01337432861328\n",
            "Epoch 5 loss -> 1792.6998291015625\n",
            "Epoch 5 loss -> 1128.40625\n",
            "Epoch 5 loss -> 446.71539306640625\n",
            "Epoch 5 loss -> 648.6956787109375\n",
            "Epoch 5 loss -> 1552.9739990234375\n",
            "Epoch 5 loss -> 244.33592224121094\n",
            "Epoch 6 loss -> 850.4544067382812\n",
            "Epoch 6 loss -> 21.06136131286621\n",
            "Epoch 6 loss -> 16.05714988708496\n",
            "Epoch 6 loss -> 21.893085479736328\n",
            "Epoch 6 loss -> 0.23370355367660522\n",
            "Epoch 6 loss -> 182.3896942138672\n",
            "Epoch 6 loss -> 398.70831298828125\n",
            "Epoch 6 loss -> 3.9346108436584473\n",
            "Epoch 6 loss -> 1119.63330078125\n",
            "Epoch 6 loss -> 209.6240997314453\n",
            "Epoch 7 loss -> 504.49371337890625\n",
            "Epoch 7 loss -> 923.7255249023438\n",
            "Epoch 7 loss -> 20.008817672729492\n",
            "Epoch 7 loss -> 2152.3662109375\n",
            "Epoch 7 loss -> 86.3418960571289\n",
            "Epoch 7 loss -> 4.908583164215088\n",
            "Epoch 7 loss -> 629.539306640625\n",
            "Epoch 7 loss -> 67.38714599609375\n",
            "Epoch 7 loss -> 1.0488810539245605\n",
            "Epoch 7 loss -> 14.697476387023926\n",
            "Epoch 8 loss -> 204.55860900878906\n",
            "Epoch 8 loss -> 178.76206970214844\n",
            "Epoch 8 loss -> 584.7879028320312\n",
            "Epoch 8 loss -> 59.67666244506836\n",
            "Epoch 8 loss -> 0.3447883427143097\n",
            "Epoch 8 loss -> 191.6988525390625\n",
            "Epoch 8 loss -> 130.19236755371094\n",
            "Epoch 8 loss -> 45.615699768066406\n",
            "Epoch 8 loss -> 61.193687438964844\n",
            "Epoch 8 loss -> 54.060279846191406\n",
            "Epoch 9 loss -> 235.99806213378906\n",
            "Epoch 9 loss -> 12.437471389770508\n",
            "Epoch 9 loss -> 818.1934204101562\n",
            "Epoch 9 loss -> 10.942621231079102\n",
            "Epoch 9 loss -> 70.44164276123047\n",
            "Epoch 9 loss -> 393.3597106933594\n",
            "Epoch 9 loss -> 50.749996185302734\n",
            "Epoch 9 loss -> 13.109667778015137\n",
            "Epoch 9 loss -> 162.93313598632812\n",
            "Epoch 9 loss -> 8.78511905670166\n",
            "Epoch 10 loss -> 121.16387176513672\n",
            "Epoch 10 loss -> 406.23687744140625\n",
            "Epoch 10 loss -> 12.048030853271484\n",
            "Epoch 10 loss -> 26.754762649536133\n",
            "Epoch 10 loss -> 103.09053039550781\n",
            "Epoch 10 loss -> 13.26340103149414\n",
            "Epoch 10 loss -> 33.446319580078125\n",
            "Epoch 10 loss -> 12.774576187133789\n",
            "Epoch 10 loss -> 10.718714714050293\n",
            "Epoch 10 loss -> 559.28759765625\n",
            "Epoch 11 loss -> 36.52420425415039\n",
            "Epoch 11 loss -> 31.19400405883789\n",
            "Epoch 11 loss -> 2.397639751434326\n",
            "Epoch 11 loss -> 144.2110137939453\n",
            "Epoch 11 loss -> 198.77919006347656\n",
            "Epoch 11 loss -> 13.631888389587402\n",
            "Epoch 11 loss -> 8.936939239501953\n",
            "Epoch 11 loss -> 29.136249542236328\n",
            "Epoch 11 loss -> 736.7203979492188\n",
            "Epoch 11 loss -> 56.385807037353516\n",
            "Epoch 12 loss -> 22.594764709472656\n",
            "Epoch 12 loss -> 13.316302299499512\n",
            "Epoch 12 loss -> 167.3471221923828\n",
            "Epoch 12 loss -> 32.644527435302734\n",
            "Epoch 12 loss -> 57.387168884277344\n",
            "Epoch 12 loss -> 21.888275146484375\n",
            "Epoch 12 loss -> 61.10813522338867\n",
            "Epoch 12 loss -> 3.680945873260498\n",
            "Epoch 12 loss -> 76.27490997314453\n",
            "Epoch 12 loss -> 4.325233459472656\n",
            "Epoch 13 loss -> 1.7995232343673706\n",
            "Epoch 13 loss -> 182.55746459960938\n",
            "Epoch 13 loss -> 8.869589805603027\n",
            "Epoch 13 loss -> 6.532120704650879\n",
            "Epoch 13 loss -> 12.108246803283691\n",
            "Epoch 13 loss -> 0.595305860042572\n",
            "Epoch 13 loss -> 6.83741569519043\n",
            "Epoch 13 loss -> 19.432659149169922\n",
            "Epoch 13 loss -> 67.21630096435547\n",
            "Epoch 13 loss -> 9.99551010131836\n",
            "Epoch 14 loss -> 7.631220817565918\n",
            "Epoch 14 loss -> 0.8343147039413452\n",
            "Epoch 14 loss -> 28.64069175720215\n",
            "Epoch 14 loss -> 210.1451416015625\n",
            "Epoch 14 loss -> 0.11908375471830368\n",
            "Epoch 14 loss -> 20.427438735961914\n",
            "Epoch 14 loss -> 1.2911670207977295\n",
            "Epoch 14 loss -> 0.6844269633293152\n",
            "Epoch 14 loss -> 0.111128069460392\n",
            "Epoch 14 loss -> 5.349843978881836\n",
            "Epoch 15 loss -> 0.6315127015113831\n",
            "Epoch 15 loss -> 8.405230522155762\n",
            "Epoch 15 loss -> 30.90472984313965\n",
            "Epoch 15 loss -> 0.8874384164810181\n",
            "Epoch 15 loss -> 150.1002655029297\n",
            "Epoch 15 loss -> 146.80035400390625\n",
            "Epoch 15 loss -> 0.5648398995399475\n",
            "Epoch 15 loss -> 0.23388062417507172\n",
            "Epoch 15 loss -> 80.95221710205078\n",
            "Epoch 15 loss -> 16.34760093688965\n",
            "Epoch 16 loss -> 0.030824661254882812\n",
            "Epoch 16 loss -> 53.052162170410156\n",
            "Epoch 16 loss -> 5.9479169845581055\n",
            "Epoch 16 loss -> 14.862217903137207\n",
            "Epoch 16 loss -> 74.46842193603516\n",
            "Epoch 16 loss -> 9.101243019104004\n",
            "Epoch 16 loss -> 31.65865135192871\n",
            "Epoch 16 loss -> 56.76869201660156\n",
            "Epoch 16 loss -> 3.8981783390045166\n",
            "Epoch 16 loss -> 1.4569244384765625\n",
            "Epoch 17 loss -> 0.35671892762184143\n",
            "Epoch 17 loss -> 2.486506938934326\n",
            "Epoch 17 loss -> 0.5270065069198608\n",
            "Epoch 17 loss -> 2.1642022132873535\n",
            "Epoch 17 loss -> 12.111909866333008\n",
            "Epoch 17 loss -> 18.322113037109375\n",
            "Epoch 17 loss -> 2.6008551120758057\n",
            "Epoch 17 loss -> 0.8203386664390564\n",
            "Epoch 17 loss -> 3.41157865524292\n",
            "Epoch 17 loss -> 16.38831329345703\n",
            "Epoch 18 loss -> 1.2387157678604126\n",
            "Epoch 18 loss -> 2.953000068664551\n",
            "Epoch 18 loss -> 0.03813207149505615\n",
            "Epoch 18 loss -> 1.1374175548553467\n",
            "Epoch 18 loss -> 3.102255344390869\n",
            "Epoch 18 loss -> 21.72374153137207\n",
            "Epoch 18 loss -> 0.7751137018203735\n",
            "Epoch 18 loss -> 17.280122756958008\n",
            "Epoch 18 loss -> 8.792062759399414\n",
            "Epoch 18 loss -> 0.6151292324066162\n",
            "Epoch 19 loss -> 1.2982420921325684\n",
            "Epoch 19 loss -> 1.3397561311721802\n",
            "Epoch 19 loss -> 14.255187034606934\n",
            "Epoch 19 loss -> 3.0677895545959473\n",
            "Epoch 19 loss -> 1.7937912940979004\n",
            "Epoch 19 loss -> 0.04810892418026924\n",
            "Epoch 19 loss -> 0.04283035546541214\n",
            "Epoch 19 loss -> 35.780609130859375\n",
            "Epoch 19 loss -> 0.037423260509967804\n",
            "Epoch 19 loss -> 2.0320255756378174\n",
            "Epoch 20 loss -> 9.95853915810585e-05\n",
            "Epoch 20 loss -> 4.889405250549316\n",
            "Epoch 20 loss -> 17.1375732421875\n",
            "Epoch 20 loss -> 6.57055139541626\n",
            "Epoch 20 loss -> 13.639325141906738\n",
            "Epoch 20 loss -> 10.870089530944824\n",
            "Epoch 20 loss -> 3.854100227355957\n",
            "Epoch 20 loss -> 17.417020797729492\n",
            "Epoch 20 loss -> 12.574585914611816\n",
            "Epoch 20 loss -> 0.8098517060279846\n",
            "Epoch 21 loss -> 0.1831492781639099\n",
            "Epoch 21 loss -> 0.9744692444801331\n",
            "Epoch 21 loss -> 10.102263450622559\n",
            "Epoch 21 loss -> 0.873393177986145\n",
            "Epoch 21 loss -> 0.7289187908172607\n",
            "Epoch 21 loss -> 3.666322708129883\n",
            "Epoch 21 loss -> 0.023747526109218597\n",
            "Epoch 21 loss -> 0.15767210721969604\n",
            "Epoch 21 loss -> 0.2369605004787445\n",
            "Epoch 21 loss -> 3.637920618057251\n",
            "Epoch 22 loss -> 0.005558365024626255\n",
            "Epoch 22 loss -> 0.084202341735363\n",
            "Epoch 22 loss -> 0.028860481455922127\n",
            "Epoch 22 loss -> 2.50673508644104\n",
            "Epoch 22 loss -> 1.695298433303833\n",
            "Epoch 22 loss -> 1.8777228593826294\n",
            "Epoch 22 loss -> 2.754750967025757\n",
            "Epoch 22 loss -> 0.004998716060072184\n",
            "Epoch 22 loss -> 7.312371730804443\n",
            "Epoch 22 loss -> 0.14514105021953583\n",
            "Epoch 23 loss -> 0.007164014503359795\n",
            "Epoch 23 loss -> 0.005055514629930258\n",
            "Epoch 23 loss -> 0.037061069160699844\n",
            "Epoch 23 loss -> 0.6911338567733765\n",
            "Epoch 23 loss -> 0.17299284040927887\n",
            "Epoch 23 loss -> 0.010621708817780018\n",
            "Epoch 23 loss -> 0.0099924486130476\n",
            "Epoch 23 loss -> 0.11869442462921143\n",
            "Epoch 23 loss -> 0.10963761806488037\n",
            "Epoch 23 loss -> 2.2001993656158447\n",
            "Epoch 24 loss -> 7.785680294036865\n",
            "Epoch 24 loss -> 1.9378291368484497\n",
            "Epoch 24 loss -> 0.025274455547332764\n",
            "Epoch 24 loss -> 0.8535429835319519\n",
            "Epoch 24 loss -> 0.00645902194082737\n",
            "Epoch 24 loss -> 0.03079185262322426\n",
            "Epoch 24 loss -> 0.21993045508861542\n",
            "Epoch 24 loss -> 0.04717720299959183\n",
            "Epoch 24 loss -> 0.17524679005146027\n",
            "Epoch 24 loss -> 3.030327796936035\n",
            "Epoch 25 loss -> 1.2879008054733276\n",
            "Epoch 25 loss -> 0.034722719341516495\n",
            "Epoch 25 loss -> 0.09696131199598312\n",
            "Epoch 25 loss -> 1.6582614183425903\n",
            "Epoch 25 loss -> 0.19111263751983643\n",
            "Epoch 25 loss -> 0.10966794192790985\n",
            "Epoch 25 loss -> 1.943568468093872\n",
            "Epoch 25 loss -> 0.003625386394560337\n",
            "Epoch 25 loss -> 0.36960113048553467\n",
            "Epoch 25 loss -> 0.04745185747742653\n",
            "Epoch 26 loss -> 0.33647292852401733\n",
            "Epoch 26 loss -> 0.1820310801267624\n",
            "Epoch 26 loss -> 1.1297656297683716\n",
            "Epoch 26 loss -> 0.5885233879089355\n",
            "Epoch 26 loss -> 0.5612018704414368\n",
            "Epoch 26 loss -> 0.40942347049713135\n",
            "Epoch 26 loss -> 0.03657285496592522\n",
            "Epoch 26 loss -> 0.3122486174106598\n",
            "Epoch 26 loss -> 1.0464340448379517e-05\n",
            "Epoch 26 loss -> 0.5623884201049805\n",
            "Epoch 27 loss -> 0.2227553129196167\n",
            "Epoch 27 loss -> 0.04295043647289276\n",
            "Epoch 27 loss -> 0.06657157093286514\n",
            "Epoch 27 loss -> 0.6036585569381714\n",
            "Epoch 27 loss -> 0.5529535412788391\n",
            "Epoch 27 loss -> 0.012202737852931023\n",
            "Epoch 27 loss -> 0.3072044253349304\n",
            "Epoch 27 loss -> 1.9278587102890015\n",
            "Epoch 27 loss -> 0.00033457798417657614\n",
            "Epoch 27 loss -> 0.8867629170417786\n",
            "Epoch 28 loss -> 0.056194085627794266\n",
            "Epoch 28 loss -> 1.288853406906128\n",
            "Epoch 28 loss -> 0.9818337559700012\n",
            "Epoch 28 loss -> 0.0005386399570852518\n",
            "Epoch 28 loss -> 0.013100216165184975\n",
            "Epoch 28 loss -> 0.03485363721847534\n",
            "Epoch 28 loss -> 0.043372053653001785\n",
            "Epoch 28 loss -> 0.43536317348480225\n",
            "Epoch 28 loss -> 0.0013642538106068969\n",
            "Epoch 28 loss -> 0.17266297340393066\n",
            "Epoch 29 loss -> 0.062046874314546585\n",
            "Epoch 29 loss -> 0.01937616802752018\n",
            "Epoch 29 loss -> 0.3012039363384247\n",
            "Epoch 29 loss -> 0.10308943688869476\n",
            "Epoch 29 loss -> 0.169684037566185\n",
            "Epoch 29 loss -> 0.027074448764324188\n",
            "Epoch 29 loss -> 0.005219288635998964\n",
            "Epoch 29 loss -> 0.0840253233909607\n",
            "Epoch 29 loss -> 0.07278288155794144\n",
            "Epoch 29 loss -> 0.014492819085717201\n",
            "Epoch 30 loss -> 0.07282816618680954\n",
            "Epoch 30 loss -> 5.93776348978281e-05\n",
            "Epoch 30 loss -> 0.005160752683877945\n",
            "Epoch 30 loss -> 0.07780231535434723\n",
            "Epoch 30 loss -> 0.10778620839118958\n",
            "Epoch 30 loss -> 0.005448564421385527\n",
            "Epoch 30 loss -> 0.02231544256210327\n",
            "Epoch 30 loss -> 0.012065749615430832\n",
            "Epoch 30 loss -> 0.32064321637153625\n",
            "Epoch 30 loss -> 0.0007746219635009766\n",
            "Epoch 31 loss -> 6.70786394039169e-05\n",
            "Epoch 31 loss -> 0.04858870804309845\n",
            "Epoch 31 loss -> 0.017644226551055908\n",
            "Epoch 31 loss -> 0.04417334496974945\n",
            "Epoch 31 loss -> 0.013175860047340393\n",
            "Epoch 31 loss -> 0.0551697202026844\n",
            "Epoch 31 loss -> 0.3028223514556885\n",
            "Epoch 31 loss -> 0.0026568034663796425\n",
            "Epoch 31 loss -> 0.21603715419769287\n",
            "Epoch 31 loss -> 0.07903720438480377\n",
            "Epoch 32 loss -> 0.08079278469085693\n",
            "Epoch 32 loss -> 0.07034603506326675\n",
            "Epoch 32 loss -> 0.013519171625375748\n",
            "Epoch 32 loss -> 0.15871596336364746\n",
            "Epoch 32 loss -> 0.11098568886518478\n",
            "Epoch 32 loss -> 0.14016760885715485\n",
            "Epoch 32 loss -> 0.037093386054039\n",
            "Epoch 32 loss -> 0.001428539166226983\n",
            "Epoch 32 loss -> 0.00012983464694116265\n",
            "Epoch 32 loss -> 0.004200557246804237\n",
            "Epoch 33 loss -> 0.007864866405725479\n",
            "Epoch 33 loss -> 0.00549314497038722\n",
            "Epoch 33 loss -> 0.033633869141340256\n",
            "Epoch 33 loss -> 0.02249816805124283\n",
            "Epoch 33 loss -> 0.01340586319565773\n",
            "Epoch 33 loss -> 0.1368512213230133\n",
            "Epoch 33 loss -> 0.015157905407249928\n",
            "Epoch 33 loss -> 0.06522998213768005\n",
            "Epoch 33 loss -> 0.018579533323645592\n",
            "Epoch 33 loss -> 0.009951688349246979\n",
            "Epoch 34 loss -> 0.0005782991647720337\n",
            "Epoch 34 loss -> 0.04778482764959335\n",
            "Epoch 34 loss -> 0.0029045678675174713\n",
            "Epoch 34 loss -> 0.0028742202557623386\n",
            "Epoch 34 loss -> 0.0062618860974907875\n",
            "Epoch 34 loss -> 0.006520483642816544\n",
            "Epoch 34 loss -> 0.036852799355983734\n",
            "Epoch 34 loss -> 0.0019141292432323098\n",
            "Epoch 34 loss -> 0.0033833421766757965\n",
            "Epoch 34 loss -> 0.008692768402397633\n",
            "Epoch 35 loss -> 0.0008020401000976562\n",
            "Epoch 35 loss -> 0.007999405264854431\n",
            "Epoch 35 loss -> 0.11317717283964157\n",
            "Epoch 35 loss -> 0.00025377050042152405\n",
            "Epoch 35 loss -> 0.09586212038993835\n",
            "Epoch 35 loss -> 0.002387166954576969\n",
            "Epoch 35 loss -> 0.002544946735724807\n",
            "Epoch 35 loss -> 0.036618832498788834\n",
            "Epoch 35 loss -> 0.00016369909280911088\n",
            "Epoch 35 loss -> 0.0007138589862734079\n",
            "Epoch 36 loss -> 6.490945816040039e-05\n",
            "Epoch 36 loss -> 0.017777234315872192\n",
            "Epoch 36 loss -> 0.01001495961099863\n",
            "Epoch 36 loss -> 0.0013534261379390955\n",
            "Epoch 36 loss -> 0.026109356433153152\n",
            "Epoch 36 loss -> 0.006102323532104492\n",
            "Epoch 36 loss -> 0.01100968848913908\n",
            "Epoch 36 loss -> 3.230670699849725e-05\n",
            "Epoch 36 loss -> 0.0007024894002825022\n",
            "Epoch 36 loss -> 0.0006681391969323158\n",
            "Epoch 37 loss -> 0.004060328006744385\n",
            "Epoch 37 loss -> 0.0014649576041847467\n",
            "Epoch 37 loss -> 0.0010051403660327196\n",
            "Epoch 37 loss -> 0.0037490769755095243\n",
            "Epoch 37 loss -> 2.6836118195205927e-05\n",
            "Epoch 37 loss -> 0.0006170906126499176\n",
            "Epoch 37 loss -> 2.295617014169693e-05\n",
            "Epoch 37 loss -> 4.146341234445572e-05\n",
            "Epoch 37 loss -> 0.022619634866714478\n",
            "Epoch 37 loss -> 0.01996466889977455\n",
            "Epoch 38 loss -> 0.031081706285476685\n",
            "Epoch 38 loss -> 0.004048668779432774\n",
            "Epoch 38 loss -> 0.0006102866027504206\n",
            "Epoch 38 loss -> 0.00442906329408288\n",
            "Epoch 38 loss -> 0.012922682799398899\n",
            "Epoch 38 loss -> 0.005298682954162359\n",
            "Epoch 38 loss -> 0.011298143304884434\n",
            "Epoch 38 loss -> 0.007422016002237797\n",
            "Epoch 38 loss -> 0.02451191283762455\n",
            "Epoch 38 loss -> 0.018924297764897346\n",
            "Epoch 39 loss -> 9.138655150309205e-05\n",
            "Epoch 39 loss -> 0.0009422977454960346\n",
            "Epoch 39 loss -> 0.003305469173938036\n",
            "Epoch 39 loss -> 0.01501499954611063\n",
            "Epoch 39 loss -> 0.00223678071051836\n",
            "Epoch 39 loss -> 0.00013532480807043612\n",
            "Epoch 39 loss -> 5.812384188175201e-06\n",
            "Epoch 39 loss -> 0.0031943470239639282\n",
            "Epoch 39 loss -> 0.0054384320974349976\n",
            "Epoch 39 loss -> 0.00035224948078393936\n",
            "Epoch 40 loss -> 0.025231413543224335\n",
            "Epoch 40 loss -> 0.002498779445886612\n",
            "Epoch 40 loss -> 0.0021122731268405914\n",
            "Epoch 40 loss -> 0.0016872920095920563\n",
            "Epoch 40 loss -> 0.0014089979231357574\n",
            "Epoch 40 loss -> 0.00037111021811142564\n",
            "Epoch 40 loss -> 0.018179770559072495\n",
            "Epoch 40 loss -> 0.009261595085263252\n",
            "Epoch 40 loss -> 0.0009150957339443266\n",
            "Epoch 40 loss -> 0.0014233533293008804\n",
            "Epoch 41 loss -> 0.00023190025240182877\n",
            "Epoch 41 loss -> 0.001459851861000061\n",
            "Epoch 41 loss -> 0.00547788804396987\n",
            "Epoch 41 loss -> 0.003263711929321289\n",
            "Epoch 41 loss -> 3.379792906343937e-05\n",
            "Epoch 41 loss -> 5.272613270790316e-05\n",
            "Epoch 41 loss -> 9.656324982643127e-05\n",
            "Epoch 41 loss -> 0.0001531848101876676\n",
            "Epoch 41 loss -> 0.0013072346337139606\n",
            "Epoch 41 loss -> 0.0012661851942539215\n",
            "Epoch 42 loss -> 0.00013228018360678107\n",
            "Epoch 42 loss -> 0.00025571882724761963\n",
            "Epoch 42 loss -> 0.006346693262457848\n",
            "Epoch 42 loss -> 0.007231330499053001\n",
            "Epoch 42 loss -> 0.000707350904121995\n",
            "Epoch 42 loss -> 2.581835724413395e-05\n",
            "Epoch 42 loss -> 0.001809779554605484\n",
            "Epoch 42 loss -> 2.9053902835585177e-05\n",
            "Epoch 42 loss -> 0.0040254006162285805\n",
            "Epoch 42 loss -> 0.00025110371643677354\n",
            "Epoch 43 loss -> 2.3283064365386963e-10\n",
            "Epoch 43 loss -> 0.0004230774939060211\n",
            "Epoch 43 loss -> 0.0009031338267959654\n",
            "Epoch 43 loss -> 1.0662712156772614e-05\n",
            "Epoch 43 loss -> 0.00030845426954329014\n",
            "Epoch 43 loss -> 0.0005775655154138803\n",
            "Epoch 43 loss -> 0.00029809126863256097\n",
            "Epoch 43 loss -> 0.0013122046366333961\n",
            "Epoch 43 loss -> 0.0024035966489464045\n",
            "Epoch 43 loss -> 0.0007077567861415446\n",
            "Epoch 44 loss -> 0.002392388414591551\n",
            "Epoch 44 loss -> 1.6598263755440712e-05\n",
            "Epoch 44 loss -> 0.0033338218927383423\n",
            "Epoch 44 loss -> 5.762703949585557e-05\n",
            "Epoch 44 loss -> 0.0010842839255928993\n",
            "Epoch 44 loss -> 0.0027424246072769165\n",
            "Epoch 44 loss -> 0.0009585267398506403\n",
            "Epoch 44 loss -> 0.00011335477756801993\n",
            "Epoch 44 loss -> 0.00039287866093218327\n",
            "Epoch 44 loss -> 0.00031329691410064697\n",
            "Epoch 45 loss -> 3.568647662177682e-05\n",
            "Epoch 45 loss -> 1.8339051166549325e-08\n",
            "Epoch 45 loss -> 3.0174851417541504e-05\n",
            "Epoch 45 loss -> 0.0005051793996244669\n",
            "Epoch 45 loss -> 7.501861546188593e-06\n",
            "Epoch 45 loss -> 0.002871766686439514\n",
            "Epoch 45 loss -> 0.0005096477107144892\n",
            "Epoch 45 loss -> 0.001193419098854065\n",
            "Epoch 45 loss -> 2.8687063604593277e-06\n",
            "Epoch 45 loss -> 0.00016980711370706558\n",
            "Epoch 46 loss -> 0.0004369998350739479\n",
            "Epoch 46 loss -> 0.0006516764406114817\n",
            "Epoch 46 loss -> 8.449482265859842e-06\n",
            "Epoch 46 loss -> 0.0005296473391354084\n",
            "Epoch 46 loss -> 4.686535248765722e-06\n",
            "Epoch 46 loss -> 4.741050361189991e-07\n",
            "Epoch 46 loss -> 0.00014092866331338882\n",
            "Epoch 46 loss -> 0.0009022169397212565\n",
            "Epoch 46 loss -> 0.0003138373140245676\n",
            "Epoch 46 loss -> 1.7864862456917763e-05\n",
            "Epoch 47 loss -> 0.00023190025240182877\n",
            "Epoch 47 loss -> 1.3160766684450209e-05\n",
            "Epoch 47 loss -> 0.00030578020960092545\n",
            "Epoch 47 loss -> 0.0015044966712594032\n",
            "Epoch 47 loss -> 0.0007960016373544931\n",
            "Epoch 47 loss -> 0.00010234513320028782\n",
            "Epoch 47 loss -> 0.00012715370394289494\n",
            "Epoch 47 loss -> 0.00023190025240182877\n",
            "Epoch 47 loss -> 0.00022088084369897842\n",
            "Epoch 47 loss -> 0.00034846513881348073\n",
            "Epoch 48 loss -> 8.734787115827203e-05\n",
            "Epoch 48 loss -> 2.3283064365386963e-06\n",
            "Epoch 48 loss -> 0.00010069241398014128\n",
            "Epoch 48 loss -> 0.0001373291015625\n",
            "Epoch 48 loss -> 0.00017622951418161392\n",
            "Epoch 48 loss -> 1.7331783965346403e-05\n",
            "Epoch 48 loss -> 0.0006921140011399984\n",
            "Epoch 48 loss -> 0.00015163072384893894\n",
            "Epoch 48 loss -> 1.1408701539039612e-06\n",
            "Epoch 48 loss -> 0.00014530960470438004\n",
            "Epoch 49 loss -> 6.055925041437149e-05\n",
            "Epoch 49 loss -> 0.00013661477714776993\n",
            "Epoch 49 loss -> 0.00011085066944360733\n",
            "Epoch 49 loss -> 2.2701002308167517e-05\n",
            "Epoch 49 loss -> 1.52587890625e-05\n",
            "Epoch 49 loss -> 9.761570254340768e-07\n",
            "Epoch 49 loss -> 7.197633385658264e-05\n",
            "Epoch 49 loss -> 1.2316741049289703e-05\n",
            "Epoch 49 loss -> 1.985207200050354e-05\n",
            "Epoch 49 loss -> 6.726622814312577e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxWeyJw5lAqU"
      },
      "source": [
        "3.1.2 Воспользовавшись классами `Linear` и `MSELoss` из задачи 2.1.4 и 2.3.1, `ReLU` из 2.2.1 и автоматическим дифференцированием, которое предоставляет `torch`, решить задачу регрессии. Для оптимизации использовать пакетный градиентный спуск. Вывести график функции потерь в зависимости от номера эпохи. Вывести на одном графике исходные данные и предсказанные значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bnlAt1NEQoat"
      },
      "outputs": [],
      "source": [
        "X = torch.FloatTensor(torch.linspace(0, 1, 100).view(-1, 1))\n",
        "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def forward(self, inputs):\n",
        "        inputs[inputs < 0] = 0\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "gN9cJO_ExFZc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELoss:\n",
        "    \"\"\"\n",
        "    MSELoss - это самый просто лосс, формула которого (output-target) ** 2/n . MSELoss решает проблемы регрессии.\n",
        "    \"\"\"\n",
        "    def backward(self):\n",
        "        self.z.backward()\n",
        "        self.dinput = self.y_pred.grad\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.y_pred = torch.tensor(y_pred, requires_grad=True)\n",
        "        y_true = torch.tensor(y_true)\n",
        "        z = ((self.y_pred - y_true) ** 2).mean()\n",
        "        self.z = z\n",
        "        return z"
      ],
      "metadata": {
        "id": "vh73GMHvxH9A"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    \n",
        "    \"\"\"Линейная регрессия\"\"\"\n",
        "    \n",
        "    def __init__(self, n_features, n_neurons):\n",
        "        self.weights = torch.randn(n_features, n_neurons)\n",
        "        self.biases = torch.randn(n_neurons)\n",
        "        \n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = torch.matmul(self.inputs.T, dvalues)\n",
        "        self.dbiases = torch.sum(dvalues, axis=0)\n",
        "        self.dinputs = torch.matmul(dvalues, self.weights.T)\n",
        "        return self.dweights, self.dbiases\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        return torch.matmul(inputs, self.weights) + self.biases"
      ],
      "metadata": {
        "id": "RDgsnBmdxJ-f"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <размерность элемента выборки >\n",
        "n_inputs = 1\n",
        "#  скорость обучения\n",
        "learning_rate = 0.01\n",
        "#  количество эпох\n",
        "n_epoch = 500\n",
        "\n",
        "neuron = Linear(1, 1)\n",
        "loss = MSELoss()\n",
        "activation = ReLU()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(n_epoch):\n",
        "    y_pred = activation.forward(neuron.forward(X))\n",
        "\n",
        "    curr_loss = loss.forward(y_pred, y)\n",
        "    #print(loss.z)\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    dweights, dbias = neuron.backward(loss.dinput)\n",
        "    #print(loss.dinput)\n",
        "\n",
        "    # update weights\n",
        "    neuron.weights -= learning_rate * dweights\n",
        "    neuron.biases -= learning_rate * dbias\n",
        "    print(f\"Epoch {epoch} loss: {curr_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXCc5b3oxMCu",
        "outputId": "7886cc11-a4c8-44a5-a97a-d45d9992f2b1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.4986480474472046\n",
            "Epoch 1 loss: 0.4986480474472046\n",
            "Epoch 2 loss: 0.4986480474472046\n",
            "Epoch 3 loss: 0.4986480474472046\n",
            "Epoch 4 loss: 0.4986480474472046\n",
            "Epoch 5 loss: 0.4986480474472046\n",
            "Epoch 6 loss: 0.4986480474472046\n",
            "Epoch 7 loss: 0.4986480474472046\n",
            "Epoch 8 loss: 0.4986480474472046\n",
            "Epoch 9 loss: 0.4986480474472046\n",
            "Epoch 10 loss: 0.4986480474472046\n",
            "Epoch 11 loss: 0.4986480474472046\n",
            "Epoch 12 loss: 0.4986480474472046\n",
            "Epoch 13 loss: 0.4986480474472046\n",
            "Epoch 14 loss: 0.4986480474472046\n",
            "Epoch 15 loss: 0.4986480474472046\n",
            "Epoch 16 loss: 0.4986480474472046\n",
            "Epoch 17 loss: 0.4986480474472046\n",
            "Epoch 18 loss: 0.4986480474472046\n",
            "Epoch 19 loss: 0.4986480474472046\n",
            "Epoch 20 loss: 0.4986480474472046\n",
            "Epoch 21 loss: 0.4986480474472046\n",
            "Epoch 22 loss: 0.4986480474472046\n",
            "Epoch 23 loss: 0.4986480474472046\n",
            "Epoch 24 loss: 0.4986480474472046\n",
            "Epoch 25 loss: 0.4986480474472046\n",
            "Epoch 26 loss: 0.4986480474472046\n",
            "Epoch 27 loss: 0.4986480474472046\n",
            "Epoch 28 loss: 0.4986480474472046\n",
            "Epoch 29 loss: 0.4986480474472046\n",
            "Epoch 30 loss: 0.4986480474472046\n",
            "Epoch 31 loss: 0.4986480474472046\n",
            "Epoch 32 loss: 0.4986480474472046\n",
            "Epoch 33 loss: 0.4986480474472046\n",
            "Epoch 34 loss: 0.4986480474472046\n",
            "Epoch 35 loss: 0.4986480474472046\n",
            "Epoch 36 loss: 0.4986480474472046\n",
            "Epoch 37 loss: 0.4986480474472046\n",
            "Epoch 38 loss: 0.4986480474472046\n",
            "Epoch 39 loss: 0.4986480474472046\n",
            "Epoch 40 loss: 0.4986480474472046\n",
            "Epoch 41 loss: 0.4986480474472046\n",
            "Epoch 42 loss: 0.4986480474472046\n",
            "Epoch 43 loss: 0.4986480474472046\n",
            "Epoch 44 loss: 0.4986480474472046\n",
            "Epoch 45 loss: 0.4986480474472046\n",
            "Epoch 46 loss: 0.4986480474472046\n",
            "Epoch 47 loss: 0.4986480474472046\n",
            "Epoch 48 loss: 0.4986480474472046\n",
            "Epoch 49 loss: 0.4986480474472046\n",
            "Epoch 50 loss: 0.4986480474472046\n",
            "Epoch 51 loss: 0.4986480474472046\n",
            "Epoch 52 loss: 0.4986480474472046\n",
            "Epoch 53 loss: 0.4986480474472046\n",
            "Epoch 54 loss: 0.4986480474472046\n",
            "Epoch 55 loss: 0.4986480474472046\n",
            "Epoch 56 loss: 0.4986480474472046\n",
            "Epoch 57 loss: 0.4986480474472046\n",
            "Epoch 58 loss: 0.4986480474472046\n",
            "Epoch 59 loss: 0.4986480474472046\n",
            "Epoch 60 loss: 0.4986480474472046\n",
            "Epoch 61 loss: 0.4986480474472046\n",
            "Epoch 62 loss: 0.4986480474472046\n",
            "Epoch 63 loss: 0.4986480474472046\n",
            "Epoch 64 loss: 0.4986480474472046\n",
            "Epoch 65 loss: 0.4986480474472046\n",
            "Epoch 66 loss: 0.4986480474472046\n",
            "Epoch 67 loss: 0.4986480474472046\n",
            "Epoch 68 loss: 0.4986480474472046\n",
            "Epoch 69 loss: 0.4986480474472046\n",
            "Epoch 70 loss: 0.4986480474472046\n",
            "Epoch 71 loss: 0.4986480474472046\n",
            "Epoch 72 loss: 0.4986480474472046\n",
            "Epoch 73 loss: 0.4986480474472046\n",
            "Epoch 74 loss: 0.4986480474472046\n",
            "Epoch 75 loss: 0.4986480474472046\n",
            "Epoch 76 loss: 0.4986480474472046\n",
            "Epoch 77 loss: 0.4986480474472046\n",
            "Epoch 78 loss: 0.4986480474472046\n",
            "Epoch 79 loss: 0.4986480474472046\n",
            "Epoch 80 loss: 0.4986480474472046\n",
            "Epoch 81 loss: 0.4986480474472046\n",
            "Epoch 82 loss: 0.4986480474472046\n",
            "Epoch 83 loss: 0.4986480474472046\n",
            "Epoch 84 loss: 0.4986480474472046\n",
            "Epoch 85 loss: 0.4986480474472046\n",
            "Epoch 86 loss: 0.4986480474472046\n",
            "Epoch 87 loss: 0.4986480474472046\n",
            "Epoch 88 loss: 0.4986480474472046\n",
            "Epoch 89 loss: 0.4986480474472046\n",
            "Epoch 90 loss: 0.4986480474472046\n",
            "Epoch 91 loss: 0.4986480474472046\n",
            "Epoch 92 loss: 0.4986480474472046\n",
            "Epoch 93 loss: 0.4986480474472046\n",
            "Epoch 94 loss: 0.4986480474472046\n",
            "Epoch 95 loss: 0.4986480474472046\n",
            "Epoch 96 loss: 0.4986480474472046\n",
            "Epoch 97 loss: 0.4986480474472046\n",
            "Epoch 98 loss: 0.4986480474472046\n",
            "Epoch 99 loss: 0.4986480474472046\n",
            "Epoch 100 loss: 0.4986480474472046\n",
            "Epoch 101 loss: 0.4986480474472046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-e790bcbd3284>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.y_pred = torch.tensor(y_pred, requires_grad=True)\n",
            "<ipython-input-52-e790bcbd3284>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_true = torch.tensor(y_true)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 102 loss: 0.4986480474472046\n",
            "Epoch 103 loss: 0.4986480474472046\n",
            "Epoch 104 loss: 0.4986480474472046\n",
            "Epoch 105 loss: 0.4986480474472046\n",
            "Epoch 106 loss: 0.4986480474472046\n",
            "Epoch 107 loss: 0.4986480474472046\n",
            "Epoch 108 loss: 0.4986480474472046\n",
            "Epoch 109 loss: 0.4986480474472046\n",
            "Epoch 110 loss: 0.4986480474472046\n",
            "Epoch 111 loss: 0.4986480474472046\n",
            "Epoch 112 loss: 0.4986480474472046\n",
            "Epoch 113 loss: 0.4986480474472046\n",
            "Epoch 114 loss: 0.4986480474472046\n",
            "Epoch 115 loss: 0.4986480474472046\n",
            "Epoch 116 loss: 0.4986480474472046\n",
            "Epoch 117 loss: 0.4986480474472046\n",
            "Epoch 118 loss: 0.4986480474472046\n",
            "Epoch 119 loss: 0.4986480474472046\n",
            "Epoch 120 loss: 0.4986480474472046\n",
            "Epoch 121 loss: 0.4986480474472046\n",
            "Epoch 122 loss: 0.4986480474472046\n",
            "Epoch 123 loss: 0.4986480474472046\n",
            "Epoch 124 loss: 0.4986480474472046\n",
            "Epoch 125 loss: 0.4986480474472046\n",
            "Epoch 126 loss: 0.4986480474472046\n",
            "Epoch 127 loss: 0.4986480474472046\n",
            "Epoch 128 loss: 0.4986480474472046\n",
            "Epoch 129 loss: 0.4986480474472046\n",
            "Epoch 130 loss: 0.4986480474472046\n",
            "Epoch 131 loss: 0.4986480474472046\n",
            "Epoch 132 loss: 0.4986480474472046\n",
            "Epoch 133 loss: 0.4986480474472046\n",
            "Epoch 134 loss: 0.4986480474472046\n",
            "Epoch 135 loss: 0.4986480474472046\n",
            "Epoch 136 loss: 0.4986480474472046\n",
            "Epoch 137 loss: 0.4986480474472046\n",
            "Epoch 138 loss: 0.4986480474472046\n",
            "Epoch 139 loss: 0.4986480474472046\n",
            "Epoch 140 loss: 0.4986480474472046\n",
            "Epoch 141 loss: 0.4986480474472046\n",
            "Epoch 142 loss: 0.4986480474472046\n",
            "Epoch 143 loss: 0.4986480474472046\n",
            "Epoch 144 loss: 0.4986480474472046\n",
            "Epoch 145 loss: 0.4986480474472046\n",
            "Epoch 146 loss: 0.4986480474472046\n",
            "Epoch 147 loss: 0.4986480474472046\n",
            "Epoch 148 loss: 0.4986480474472046\n",
            "Epoch 149 loss: 0.4986480474472046\n",
            "Epoch 150 loss: 0.4986480474472046\n",
            "Epoch 151 loss: 0.4986480474472046\n",
            "Epoch 152 loss: 0.4986480474472046\n",
            "Epoch 153 loss: 0.4986480474472046\n",
            "Epoch 154 loss: 0.4986480474472046\n",
            "Epoch 155 loss: 0.4986480474472046\n",
            "Epoch 156 loss: 0.4986480474472046\n",
            "Epoch 157 loss: 0.4986480474472046\n",
            "Epoch 158 loss: 0.4986480474472046\n",
            "Epoch 159 loss: 0.4986480474472046\n",
            "Epoch 160 loss: 0.4986480474472046\n",
            "Epoch 161 loss: 0.4986480474472046\n",
            "Epoch 162 loss: 0.4986480474472046\n",
            "Epoch 163 loss: 0.4986480474472046\n",
            "Epoch 164 loss: 0.4986480474472046\n",
            "Epoch 165 loss: 0.4986480474472046\n",
            "Epoch 166 loss: 0.4986480474472046\n",
            "Epoch 167 loss: 0.4986480474472046\n",
            "Epoch 168 loss: 0.4986480474472046\n",
            "Epoch 169 loss: 0.4986480474472046\n",
            "Epoch 170 loss: 0.4986480474472046\n",
            "Epoch 171 loss: 0.4986480474472046\n",
            "Epoch 172 loss: 0.4986480474472046\n",
            "Epoch 173 loss: 0.4986480474472046\n",
            "Epoch 174 loss: 0.4986480474472046\n",
            "Epoch 175 loss: 0.4986480474472046\n",
            "Epoch 176 loss: 0.4986480474472046\n",
            "Epoch 177 loss: 0.4986480474472046\n",
            "Epoch 178 loss: 0.4986480474472046\n",
            "Epoch 179 loss: 0.4986480474472046\n",
            "Epoch 180 loss: 0.4986480474472046\n",
            "Epoch 181 loss: 0.4986480474472046\n",
            "Epoch 182 loss: 0.4986480474472046\n",
            "Epoch 183 loss: 0.4986480474472046\n",
            "Epoch 184 loss: 0.4986480474472046\n",
            "Epoch 185 loss: 0.4986480474472046\n",
            "Epoch 186 loss: 0.4986480474472046\n",
            "Epoch 187 loss: 0.4986480474472046\n",
            "Epoch 188 loss: 0.4986480474472046\n",
            "Epoch 189 loss: 0.4986480474472046\n",
            "Epoch 190 loss: 0.4986480474472046\n",
            "Epoch 191 loss: 0.4986480474472046\n",
            "Epoch 192 loss: 0.4986480474472046\n",
            "Epoch 193 loss: 0.4986480474472046\n",
            "Epoch 194 loss: 0.4986480474472046\n",
            "Epoch 195 loss: 0.4986480474472046\n",
            "Epoch 196 loss: 0.4986480474472046\n",
            "Epoch 197 loss: 0.4986480474472046\n",
            "Epoch 198 loss: 0.4986480474472046\n",
            "Epoch 199 loss: 0.4986480474472046\n",
            "Epoch 200 loss: 0.4986480474472046\n",
            "Epoch 201 loss: 0.4986480474472046\n",
            "Epoch 202 loss: 0.4986480474472046\n",
            "Epoch 203 loss: 0.4986480474472046\n",
            "Epoch 204 loss: 0.4986480474472046\n",
            "Epoch 205 loss: 0.4986480474472046\n",
            "Epoch 206 loss: 0.4986480474472046\n",
            "Epoch 207 loss: 0.4986480474472046\n",
            "Epoch 208 loss: 0.4986480474472046\n",
            "Epoch 209 loss: 0.4986480474472046\n",
            "Epoch 210 loss: 0.4986480474472046\n",
            "Epoch 211 loss: 0.4986480474472046\n",
            "Epoch 212 loss: 0.4986480474472046\n",
            "Epoch 213 loss: 0.4986480474472046\n",
            "Epoch 214 loss: 0.4986480474472046\n",
            "Epoch 215 loss: 0.4986480474472046\n",
            "Epoch 216 loss: 0.4986480474472046\n",
            "Epoch 217 loss: 0.4986480474472046\n",
            "Epoch 218 loss: 0.4986480474472046\n",
            "Epoch 219 loss: 0.4986480474472046\n",
            "Epoch 220 loss: 0.4986480474472046\n",
            "Epoch 221 loss: 0.4986480474472046\n",
            "Epoch 222 loss: 0.4986480474472046\n",
            "Epoch 223 loss: 0.4986480474472046\n",
            "Epoch 224 loss: 0.4986480474472046\n",
            "Epoch 225 loss: 0.4986480474472046\n",
            "Epoch 226 loss: 0.4986480474472046\n",
            "Epoch 227 loss: 0.4986480474472046\n",
            "Epoch 228 loss: 0.4986480474472046\n",
            "Epoch 229 loss: 0.4986480474472046\n",
            "Epoch 230 loss: 0.4986480474472046\n",
            "Epoch 231 loss: 0.4986480474472046\n",
            "Epoch 232 loss: 0.4986480474472046\n",
            "Epoch 233 loss: 0.4986480474472046\n",
            "Epoch 234 loss: 0.4986480474472046\n",
            "Epoch 235 loss: 0.4986480474472046\n",
            "Epoch 236 loss: 0.4986480474472046\n",
            "Epoch 237 loss: 0.4986480474472046\n",
            "Epoch 238 loss: 0.4986480474472046\n",
            "Epoch 239 loss: 0.4986480474472046\n",
            "Epoch 240 loss: 0.4986480474472046\n",
            "Epoch 241 loss: 0.4986480474472046\n",
            "Epoch 242 loss: 0.4986480474472046\n",
            "Epoch 243 loss: 0.4986480474472046\n",
            "Epoch 244 loss: 0.4986480474472046\n",
            "Epoch 245 loss: 0.4986480474472046\n",
            "Epoch 246 loss: 0.4986480474472046\n",
            "Epoch 247 loss: 0.4986480474472046\n",
            "Epoch 248 loss: 0.4986480474472046\n",
            "Epoch 249 loss: 0.4986480474472046\n",
            "Epoch 250 loss: 0.4986480474472046\n",
            "Epoch 251 loss: 0.4986480474472046\n",
            "Epoch 252 loss: 0.4986480474472046\n",
            "Epoch 253 loss: 0.4986480474472046\n",
            "Epoch 254 loss: 0.4986480474472046\n",
            "Epoch 255 loss: 0.4986480474472046\n",
            "Epoch 256 loss: 0.4986480474472046\n",
            "Epoch 257 loss: 0.4986480474472046\n",
            "Epoch 258 loss: 0.4986480474472046\n",
            "Epoch 259 loss: 0.4986480474472046\n",
            "Epoch 260 loss: 0.4986480474472046\n",
            "Epoch 261 loss: 0.4986480474472046\n",
            "Epoch 262 loss: 0.4986480474472046\n",
            "Epoch 263 loss: 0.4986480474472046\n",
            "Epoch 264 loss: 0.4986480474472046\n",
            "Epoch 265 loss: 0.4986480474472046\n",
            "Epoch 266 loss: 0.4986480474472046\n",
            "Epoch 267 loss: 0.4986480474472046\n",
            "Epoch 268 loss: 0.4986480474472046\n",
            "Epoch 269 loss: 0.4986480474472046\n",
            "Epoch 270 loss: 0.4986480474472046\n",
            "Epoch 271 loss: 0.4986480474472046\n",
            "Epoch 272 loss: 0.4986480474472046\n",
            "Epoch 273 loss: 0.4986480474472046\n",
            "Epoch 274 loss: 0.4986480474472046\n",
            "Epoch 275 loss: 0.4986480474472046\n",
            "Epoch 276 loss: 0.4986480474472046\n",
            "Epoch 277 loss: 0.4986480474472046\n",
            "Epoch 278 loss: 0.4986480474472046\n",
            "Epoch 279 loss: 0.4986480474472046\n",
            "Epoch 280 loss: 0.4986480474472046\n",
            "Epoch 281 loss: 0.4986480474472046\n",
            "Epoch 282 loss: 0.4986480474472046\n",
            "Epoch 283 loss: 0.4986480474472046\n",
            "Epoch 284 loss: 0.4986480474472046\n",
            "Epoch 285 loss: 0.4986480474472046\n",
            "Epoch 286 loss: 0.4986480474472046\n",
            "Epoch 287 loss: 0.4986480474472046\n",
            "Epoch 288 loss: 0.4986480474472046\n",
            "Epoch 289 loss: 0.4986480474472046\n",
            "Epoch 290 loss: 0.4986480474472046\n",
            "Epoch 291 loss: 0.4986480474472046\n",
            "Epoch 292 loss: 0.4986480474472046\n",
            "Epoch 293 loss: 0.4986480474472046\n",
            "Epoch 294 loss: 0.4986480474472046\n",
            "Epoch 295 loss: 0.4986480474472046\n",
            "Epoch 296 loss: 0.4986480474472046\n",
            "Epoch 297 loss: 0.4986480474472046\n",
            "Epoch 298 loss: 0.4986480474472046\n",
            "Epoch 299 loss: 0.4986480474472046\n",
            "Epoch 300 loss: 0.4986480474472046\n",
            "Epoch 301 loss: 0.4986480474472046\n",
            "Epoch 302 loss: 0.4986480474472046\n",
            "Epoch 303 loss: 0.4986480474472046\n",
            "Epoch 304 loss: 0.4986480474472046\n",
            "Epoch 305 loss: 0.4986480474472046\n",
            "Epoch 306 loss: 0.4986480474472046\n",
            "Epoch 307 loss: 0.4986480474472046\n",
            "Epoch 308 loss: 0.4986480474472046\n",
            "Epoch 309 loss: 0.4986480474472046\n",
            "Epoch 310 loss: 0.4986480474472046\n",
            "Epoch 311 loss: 0.4986480474472046\n",
            "Epoch 312 loss: 0.4986480474472046\n",
            "Epoch 313 loss: 0.4986480474472046\n",
            "Epoch 314 loss: 0.4986480474472046\n",
            "Epoch 315 loss: 0.4986480474472046\n",
            "Epoch 316 loss: 0.4986480474472046\n",
            "Epoch 317 loss: 0.4986480474472046\n",
            "Epoch 318 loss: 0.4986480474472046\n",
            "Epoch 319 loss: 0.4986480474472046\n",
            "Epoch 320 loss: 0.4986480474472046\n",
            "Epoch 321 loss: 0.4986480474472046\n",
            "Epoch 322 loss: 0.4986480474472046\n",
            "Epoch 323 loss: 0.4986480474472046\n",
            "Epoch 324 loss: 0.4986480474472046\n",
            "Epoch 325 loss: 0.4986480474472046\n",
            "Epoch 326 loss: 0.4986480474472046\n",
            "Epoch 327 loss: 0.4986480474472046\n",
            "Epoch 328 loss: 0.4986480474472046\n",
            "Epoch 329 loss: 0.4986480474472046\n",
            "Epoch 330 loss: 0.4986480474472046\n",
            "Epoch 331 loss: 0.4986480474472046\n",
            "Epoch 332 loss: 0.4986480474472046\n",
            "Epoch 333 loss: 0.4986480474472046\n",
            "Epoch 334 loss: 0.4986480474472046\n",
            "Epoch 335 loss: 0.4986480474472046\n",
            "Epoch 336 loss: 0.4986480474472046\n",
            "Epoch 337 loss: 0.4986480474472046\n",
            "Epoch 338 loss: 0.4986480474472046\n",
            "Epoch 339 loss: 0.4986480474472046\n",
            "Epoch 340 loss: 0.4986480474472046\n",
            "Epoch 341 loss: 0.4986480474472046\n",
            "Epoch 342 loss: 0.4986480474472046\n",
            "Epoch 343 loss: 0.4986480474472046\n",
            "Epoch 344 loss: 0.4986480474472046\n",
            "Epoch 345 loss: 0.4986480474472046\n",
            "Epoch 346 loss: 0.4986480474472046\n",
            "Epoch 347 loss: 0.4986480474472046\n",
            "Epoch 348 loss: 0.4986480474472046\n",
            "Epoch 349 loss: 0.4986480474472046\n",
            "Epoch 350 loss: 0.4986480474472046\n",
            "Epoch 351 loss: 0.4986480474472046\n",
            "Epoch 352 loss: 0.4986480474472046\n",
            "Epoch 353 loss: 0.4986480474472046\n",
            "Epoch 354 loss: 0.4986480474472046\n",
            "Epoch 355 loss: 0.4986480474472046\n",
            "Epoch 356 loss: 0.4986480474472046\n",
            "Epoch 357 loss: 0.4986480474472046\n",
            "Epoch 358 loss: 0.4986480474472046\n",
            "Epoch 359 loss: 0.4986480474472046\n",
            "Epoch 360 loss: 0.4986480474472046\n",
            "Epoch 361 loss: 0.4986480474472046\n",
            "Epoch 362 loss: 0.4986480474472046\n",
            "Epoch 363 loss: 0.4986480474472046\n",
            "Epoch 364 loss: 0.4986480474472046\n",
            "Epoch 365 loss: 0.4986480474472046\n",
            "Epoch 366 loss: 0.4986480474472046\n",
            "Epoch 367 loss: 0.4986480474472046\n",
            "Epoch 368 loss: 0.4986480474472046\n",
            "Epoch 369 loss: 0.4986480474472046\n",
            "Epoch 370 loss: 0.4986480474472046\n",
            "Epoch 371 loss: 0.4986480474472046\n",
            "Epoch 372 loss: 0.4986480474472046\n",
            "Epoch 373 loss: 0.4986480474472046\n",
            "Epoch 374 loss: 0.4986480474472046\n",
            "Epoch 375 loss: 0.4986480474472046\n",
            "Epoch 376 loss: 0.4986480474472046\n",
            "Epoch 377 loss: 0.4986480474472046\n",
            "Epoch 378 loss: 0.4986480474472046\n",
            "Epoch 379 loss: 0.4986480474472046\n",
            "Epoch 380 loss: 0.4986480474472046\n",
            "Epoch 381 loss: 0.4986480474472046\n",
            "Epoch 382 loss: 0.4986480474472046\n",
            "Epoch 383 loss: 0.4986480474472046\n",
            "Epoch 384 loss: 0.4986480474472046\n",
            "Epoch 385 loss: 0.4986480474472046\n",
            "Epoch 386 loss: 0.4986480474472046\n",
            "Epoch 387 loss: 0.4986480474472046\n",
            "Epoch 388 loss: 0.4986480474472046\n",
            "Epoch 389 loss: 0.4986480474472046\n",
            "Epoch 390 loss: 0.4986480474472046\n",
            "Epoch 391 loss: 0.4986480474472046\n",
            "Epoch 392 loss: 0.4986480474472046\n",
            "Epoch 393 loss: 0.4986480474472046\n",
            "Epoch 394 loss: 0.4986480474472046\n",
            "Epoch 395 loss: 0.4986480474472046\n",
            "Epoch 396 loss: 0.4986480474472046\n",
            "Epoch 397 loss: 0.4986480474472046\n",
            "Epoch 398 loss: 0.4986480474472046\n",
            "Epoch 399 loss: 0.4986480474472046\n",
            "Epoch 400 loss: 0.4986480474472046\n",
            "Epoch 401 loss: 0.4986480474472046\n",
            "Epoch 402 loss: 0.4986480474472046\n",
            "Epoch 403 loss: 0.4986480474472046\n",
            "Epoch 404 loss: 0.4986480474472046\n",
            "Epoch 405 loss: 0.4986480474472046\n",
            "Epoch 406 loss: 0.4986480474472046\n",
            "Epoch 407 loss: 0.4986480474472046\n",
            "Epoch 408 loss: 0.4986480474472046\n",
            "Epoch 409 loss: 0.4986480474472046\n",
            "Epoch 410 loss: 0.4986480474472046\n",
            "Epoch 411 loss: 0.4986480474472046\n",
            "Epoch 412 loss: 0.4986480474472046\n",
            "Epoch 413 loss: 0.4986480474472046\n",
            "Epoch 414 loss: 0.4986480474472046\n",
            "Epoch 415 loss: 0.4986480474472046\n",
            "Epoch 416 loss: 0.4986480474472046\n",
            "Epoch 417 loss: 0.4986480474472046\n",
            "Epoch 418 loss: 0.4986480474472046\n",
            "Epoch 419 loss: 0.4986480474472046\n",
            "Epoch 420 loss: 0.4986480474472046\n",
            "Epoch 421 loss: 0.4986480474472046\n",
            "Epoch 422 loss: 0.4986480474472046\n",
            "Epoch 423 loss: 0.4986480474472046\n",
            "Epoch 424 loss: 0.4986480474472046\n",
            "Epoch 425 loss: 0.4986480474472046\n",
            "Epoch 426 loss: 0.4986480474472046\n",
            "Epoch 427 loss: 0.4986480474472046\n",
            "Epoch 428 loss: 0.4986480474472046\n",
            "Epoch 429 loss: 0.4986480474472046\n",
            "Epoch 430 loss: 0.4986480474472046\n",
            "Epoch 431 loss: 0.4986480474472046\n",
            "Epoch 432 loss: 0.4986480474472046\n",
            "Epoch 433 loss: 0.4986480474472046\n",
            "Epoch 434 loss: 0.4986480474472046\n",
            "Epoch 435 loss: 0.4986480474472046\n",
            "Epoch 436 loss: 0.4986480474472046\n",
            "Epoch 437 loss: 0.4986480474472046\n",
            "Epoch 438 loss: 0.4986480474472046\n",
            "Epoch 439 loss: 0.4986480474472046\n",
            "Epoch 440 loss: 0.4986480474472046\n",
            "Epoch 441 loss: 0.4986480474472046\n",
            "Epoch 442 loss: 0.4986480474472046\n",
            "Epoch 443 loss: 0.4986480474472046\n",
            "Epoch 444 loss: 0.4986480474472046\n",
            "Epoch 445 loss: 0.4986480474472046\n",
            "Epoch 446 loss: 0.4986480474472046\n",
            "Epoch 447 loss: 0.4986480474472046\n",
            "Epoch 448 loss: 0.4986480474472046\n",
            "Epoch 449 loss: 0.4986480474472046\n",
            "Epoch 450 loss: 0.4986480474472046\n",
            "Epoch 451 loss: 0.4986480474472046\n",
            "Epoch 452 loss: 0.4986480474472046\n",
            "Epoch 453 loss: 0.4986480474472046\n",
            "Epoch 454 loss: 0.4986480474472046\n",
            "Epoch 455 loss: 0.4986480474472046\n",
            "Epoch 456 loss: 0.4986480474472046\n",
            "Epoch 457 loss: 0.4986480474472046\n",
            "Epoch 458 loss: 0.4986480474472046\n",
            "Epoch 459 loss: 0.4986480474472046\n",
            "Epoch 460 loss: 0.4986480474472046\n",
            "Epoch 461 loss: 0.4986480474472046\n",
            "Epoch 462 loss: 0.4986480474472046\n",
            "Epoch 463 loss: 0.4986480474472046\n",
            "Epoch 464 loss: 0.4986480474472046\n",
            "Epoch 465 loss: 0.4986480474472046\n",
            "Epoch 466 loss: 0.4986480474472046\n",
            "Epoch 467 loss: 0.4986480474472046\n",
            "Epoch 468 loss: 0.4986480474472046\n",
            "Epoch 469 loss: 0.4986480474472046\n",
            "Epoch 470 loss: 0.4986480474472046\n",
            "Epoch 471 loss: 0.4986480474472046\n",
            "Epoch 472 loss: 0.4986480474472046\n",
            "Epoch 473 loss: 0.4986480474472046\n",
            "Epoch 474 loss: 0.4986480474472046\n",
            "Epoch 475 loss: 0.4986480474472046\n",
            "Epoch 476 loss: 0.4986480474472046\n",
            "Epoch 477 loss: 0.4986480474472046\n",
            "Epoch 478 loss: 0.4986480474472046\n",
            "Epoch 479 loss: 0.4986480474472046\n",
            "Epoch 480 loss: 0.4986480474472046\n",
            "Epoch 481 loss: 0.4986480474472046\n",
            "Epoch 482 loss: 0.4986480474472046\n",
            "Epoch 483 loss: 0.4986480474472046\n",
            "Epoch 484 loss: 0.4986480474472046\n",
            "Epoch 485 loss: 0.4986480474472046\n",
            "Epoch 486 loss: 0.4986480474472046\n",
            "Epoch 487 loss: 0.4986480474472046\n",
            "Epoch 488 loss: 0.4986480474472046\n",
            "Epoch 489 loss: 0.4986480474472046\n",
            "Epoch 490 loss: 0.4986480474472046\n",
            "Epoch 491 loss: 0.4986480474472046\n",
            "Epoch 492 loss: 0.4986480474472046\n",
            "Epoch 493 loss: 0.4986480474472046\n",
            "Epoch 494 loss: 0.4986480474472046\n",
            "Epoch 495 loss: 0.4986480474472046\n",
            "Epoch 496 loss: 0.4986480474472046\n",
            "Epoch 497 loss: 0.4986480474472046\n",
            "Epoch 498 loss: 0.4986480474472046\n",
            "Epoch 499 loss: 0.4986480474472046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nifm0FVB2y5N"
      },
      "source": [
        "## 3.2 Алгоритмы оптимизации в `torch.optim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "u5PTTYou3xx8"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oBFfJpmcwfn"
      },
      "source": [
        "3.2.1 Решить задачу 3.1.1, воспользовавшись оптимизатором `optim.SDG` для применения стохастического градиентого спуска"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
        "X = torch.from_numpy(X).to(dtype=torch.float32)\n",
        "y = torch.from_numpy(y).to(dtype=torch.float32)"
      ],
      "metadata": {
        "id": "PCgFeLKoyx_J"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    def __init__(self, n_inputs):\n",
        "        # <создать атрибуты объекта weights и bias>\n",
        "        # Создаем случайные веса и смещение нужных размерностей\n",
        "        self.W = torch.nn.Parameter(torch.randn(1, n_inputs, requires_grad=True))\n",
        "        self.B = torch.nn.Parameter(torch.randn(1, requires_grad=True))\n",
        "      \n",
        "    def parameters(self):\n",
        "        yield self.W\n",
        "        yield self.B\n",
        "  \n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        return torch.matmul(inputs, self.W.T) + self.B\n",
        "      \n",
        "    def backward(self, dvalue):\n",
        "        with torch.no_grad():\n",
        "            self.dweights = dvalue * self.inputs\n",
        "            self.dinput =  dvalue * self.W\n",
        "            self.dbias = dvalue \n",
        "        \n",
        "        # Возвращаем градиент весов и смещения\n",
        "        return self.dweights, self.dbias"
      ],
      "metadata": {
        "id": "mrXeqp9Uyz-B"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SquaredLoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "        self.loss = (y_pred - y_true)**2\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self):\n",
        "        y_pred = torch.autograd.Variable(self.y_pred, requires_grad=True)\n",
        "        y_true = torch.autograd.Variable(self.y_true, requires_grad=False)\n",
        "        ((y_pred - y_true)**2).backward()\n",
        "        self.dinput = y_pred.grad"
      ],
      "metadata": {
        "id": "xXtZJHl8y1wC"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <размерность элемента выборки >\n",
        "n_inputs = 4\n",
        "#  скорость обучения\n",
        "learning_rate = 0.0001\n",
        "n_epoch = 500 #  количество эпох\n",
        "batch_size = 10\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "optimizer = optim.SGD(neuron.parameters(), lr=learning_rate)\n",
        "# print(optimizer.param_groups)\n",
        "loss = SquaredLoss()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(n_epoch):\n",
        "    for x_example, y_example in zip(X, y):\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        y_pred = neuron.forward(x_example)\n",
        "        curr_loss = loss.forward(y_pred, y_example)\n",
        "        losses.append(curr_loss)\n",
        "\n",
        "        # backward pass\n",
        "        optimizer = torch.optim.SGD(neuron.parameters(), learning_rate)\n",
        "        optimizer.zero_grad()\n",
        "        loss.forward(y_pred, y_example).backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch} loss: {curr_loss[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMp4g1Rpy3nc",
        "outputId": "062c5990-7803-415b-d0df-c53ad348f160"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 32565.76953125\n",
            "Epoch 1 loss: 31323.447265625\n",
            "Epoch 2 loss: 30129.65234375\n",
            "Epoch 3 loss: 28982.443359375\n",
            "Epoch 4 loss: 27879.970703125\n",
            "Epoch 5 loss: 26820.4453125\n",
            "Epoch 6 loss: 25802.16015625\n",
            "Epoch 7 loss: 24823.470703125\n",
            "Epoch 8 loss: 23882.806640625\n",
            "Epoch 9 loss: 22978.658203125\n",
            "Epoch 10 loss: 22109.5703125\n",
            "Epoch 11 loss: 21274.15625\n",
            "Epoch 12 loss: 20471.083984375\n",
            "Epoch 13 loss: 19699.0703125\n",
            "Epoch 14 loss: 18956.888671875\n",
            "Epoch 15 loss: 18243.36328125\n",
            "Epoch 16 loss: 17557.349609375\n",
            "Epoch 17 loss: 16897.775390625\n",
            "Epoch 18 loss: 16263.5927734375\n",
            "Epoch 19 loss: 15653.80078125\n",
            "Epoch 20 loss: 15067.4462890625\n",
            "Epoch 21 loss: 14503.599609375\n",
            "Epoch 22 loss: 13961.384765625\n",
            "Epoch 23 loss: 13439.94140625\n",
            "Epoch 24 loss: 12938.466796875\n",
            "Epoch 25 loss: 12456.1669921875\n",
            "Epoch 26 loss: 11992.2998046875\n",
            "Epoch 27 loss: 11546.1474609375\n",
            "Epoch 28 loss: 11117.0\n",
            "Epoch 29 loss: 10704.2197265625\n",
            "Epoch 30 loss: 10307.1494140625\n",
            "Epoch 31 loss: 9925.181640625\n",
            "Epoch 32 loss: 9557.732421875\n",
            "Epoch 33 loss: 9204.2333984375\n",
            "Epoch 34 loss: 8864.1396484375\n",
            "Epoch 35 loss: 8536.9365234375\n",
            "Epoch 36 loss: 8222.1142578125\n",
            "Epoch 37 loss: 7919.2021484375\n",
            "Epoch 38 loss: 7627.72607421875\n",
            "Epoch 39 loss: 7347.25537109375\n",
            "Epoch 40 loss: 7077.36865234375\n",
            "Epoch 41 loss: 6817.6484375\n",
            "Epoch 42 loss: 6567.69580078125\n",
            "Epoch 43 loss: 6327.15283203125\n",
            "Epoch 44 loss: 6095.650390625\n",
            "Epoch 45 loss: 5872.830078125\n",
            "Epoch 46 loss: 5658.37353515625\n",
            "Epoch 47 loss: 5451.9453125\n",
            "Epoch 48 loss: 5253.23828125\n",
            "Epoch 49 loss: 5061.96337890625\n",
            "Epoch 50 loss: 4877.833984375\n",
            "Epoch 51 loss: 4700.580078125\n",
            "Epoch 52 loss: 4529.9365234375\n",
            "Epoch 53 loss: 4365.64697265625\n",
            "Epoch 54 loss: 4207.466796875\n",
            "Epoch 55 loss: 4055.170654296875\n",
            "Epoch 56 loss: 3908.529541015625\n",
            "Epoch 57 loss: 3767.332763671875\n",
            "Epoch 58 loss: 3631.365234375\n",
            "Epoch 59 loss: 3500.43505859375\n",
            "Epoch 60 loss: 3374.347412109375\n",
            "Epoch 61 loss: 3252.920166015625\n",
            "Epoch 62 loss: 3135.981201171875\n",
            "Epoch 63 loss: 3023.35205078125\n",
            "Epoch 64 loss: 2914.872802734375\n",
            "Epoch 65 loss: 2810.39111328125\n",
            "Epoch 66 loss: 2709.749267578125\n",
            "Epoch 67 loss: 2612.807861328125\n",
            "Epoch 68 loss: 2519.42236328125\n",
            "Epoch 69 loss: 2429.46728515625\n",
            "Epoch 70 loss: 2342.80615234375\n",
            "Epoch 71 loss: 2259.318359375\n",
            "Epoch 72 loss: 2178.885009765625\n",
            "Epoch 73 loss: 2101.386474609375\n",
            "Epoch 74 loss: 2026.7142333984375\n",
            "Epoch 75 loss: 1954.767578125\n",
            "Epoch 76 loss: 1885.4420166015625\n",
            "Epoch 77 loss: 1818.640625\n",
            "Epoch 78 loss: 1754.2684326171875\n",
            "Epoch 79 loss: 1692.234619140625\n",
            "Epoch 80 loss: 1632.450439453125\n",
            "Epoch 81 loss: 1574.8372802734375\n",
            "Epoch 82 loss: 1519.3092041015625\n",
            "Epoch 83 loss: 1465.79296875\n",
            "Epoch 84 loss: 1414.2103271484375\n",
            "Epoch 85 loss: 1364.48486328125\n",
            "Epoch 86 loss: 1316.5572509765625\n",
            "Epoch 87 loss: 1270.3585205078125\n",
            "Epoch 88 loss: 1225.8204345703125\n",
            "Epoch 89 loss: 1182.88720703125\n",
            "Epoch 90 loss: 1141.49755859375\n",
            "Epoch 91 loss: 1101.593505859375\n",
            "Epoch 92 loss: 1063.1175537109375\n",
            "Epoch 93 loss: 1026.02734375\n",
            "Epoch 94 loss: 990.2630004882812\n",
            "Epoch 95 loss: 955.7764282226562\n",
            "Epoch 96 loss: 922.5228881835938\n",
            "Epoch 97 loss: 890.4581909179688\n",
            "Epoch 98 loss: 859.5339965820312\n",
            "Epoch 99 loss: 829.7091674804688\n",
            "Epoch 100 loss: 800.9480590820312\n",
            "Epoch 101 loss: 773.2084350585938\n",
            "Epoch 102 loss: 746.455322265625\n",
            "Epoch 103 loss: 720.6565551757812\n",
            "Epoch 104 loss: 695.7670288085938\n",
            "Epoch 105 loss: 671.7608642578125\n",
            "Epoch 106 loss: 648.6043701171875\n",
            "Epoch 107 loss: 626.2686767578125\n",
            "Epoch 108 loss: 604.7197265625\n",
            "Epoch 109 loss: 583.9343872070312\n",
            "Epoch 110 loss: 563.879150390625\n",
            "Epoch 111 loss: 544.5303955078125\n",
            "Epoch 112 loss: 525.8615112304688\n",
            "Epoch 113 loss: 507.8525085449219\n",
            "Epoch 114 loss: 490.4735412597656\n",
            "Epoch 115 loss: 473.7052307128906\n",
            "Epoch 116 loss: 457.52227783203125\n",
            "Epoch 117 loss: 441.9085693359375\n",
            "Epoch 118 loss: 426.842041015625\n",
            "Epoch 119 loss: 412.3013916015625\n",
            "Epoch 120 loss: 398.2733459472656\n",
            "Epoch 121 loss: 384.7310791015625\n",
            "Epoch 122 loss: 371.66009521484375\n",
            "Epoch 123 loss: 359.0451354980469\n",
            "Epoch 124 loss: 346.87017822265625\n",
            "Epoch 125 loss: 335.1163330078125\n",
            "Epoch 126 loss: 323.7715148925781\n",
            "Epoch 127 loss: 312.82012939453125\n",
            "Epoch 128 loss: 302.2486572265625\n",
            "Epoch 129 loss: 292.0429382324219\n",
            "Epoch 130 loss: 282.19036865234375\n",
            "Epoch 131 loss: 272.6781921386719\n",
            "Epoch 132 loss: 263.4954833984375\n",
            "Epoch 133 loss: 254.62879943847656\n",
            "Epoch 134 loss: 246.0708465576172\n",
            "Epoch 135 loss: 237.80458068847656\n",
            "Epoch 136 loss: 229.82386779785156\n",
            "Epoch 137 loss: 222.11805725097656\n",
            "Epoch 138 loss: 214.67642211914062\n",
            "Epoch 139 loss: 207.49124145507812\n",
            "Epoch 140 loss: 200.551513671875\n",
            "Epoch 141 loss: 193.8500518798828\n",
            "Epoch 142 loss: 187.3773193359375\n",
            "Epoch 143 loss: 181.12619018554688\n",
            "Epoch 144 loss: 175.08729553222656\n",
            "Epoch 145 loss: 169.25717163085938\n",
            "Epoch 146 loss: 163.62570190429688\n",
            "Epoch 147 loss: 158.1842803955078\n",
            "Epoch 148 loss: 152.92808532714844\n",
            "Epoch 149 loss: 147.85232543945312\n",
            "Epoch 150 loss: 142.94833374023438\n",
            "Epoch 151 loss: 138.2102813720703\n",
            "Epoch 152 loss: 133.633544921875\n",
            "Epoch 153 loss: 129.2126007080078\n",
            "Epoch 154 loss: 124.94140625\n",
            "Epoch 155 loss: 120.81476593017578\n",
            "Epoch 156 loss: 116.82635498046875\n",
            "Epoch 157 loss: 112.97235107421875\n",
            "Epoch 158 loss: 109.25027465820312\n",
            "Epoch 159 loss: 105.65359497070312\n",
            "Epoch 160 loss: 102.17725372314453\n",
            "Epoch 161 loss: 98.81851196289062\n",
            "Epoch 162 loss: 95.57228088378906\n",
            "Epoch 163 loss: 92.43424987792969\n",
            "Epoch 164 loss: 89.40312194824219\n",
            "Epoch 165 loss: 86.47359466552734\n",
            "Epoch 166 loss: 83.64254760742188\n",
            "Epoch 167 loss: 80.9052734375\n",
            "Epoch 168 loss: 78.2602310180664\n",
            "Epoch 169 loss: 75.70295715332031\n",
            "Epoch 170 loss: 73.23229217529297\n",
            "Epoch 171 loss: 70.84293365478516\n",
            "Epoch 172 loss: 68.53361511230469\n",
            "Epoch 173 loss: 66.30083465576172\n",
            "Epoch 174 loss: 64.14289855957031\n",
            "Epoch 175 loss: 62.05720901489258\n",
            "Epoch 176 loss: 60.040035247802734\n",
            "Epoch 177 loss: 58.09083938598633\n",
            "Epoch 178 loss: 56.206520080566406\n",
            "Epoch 179 loss: 54.38432312011719\n",
            "Epoch 180 loss: 52.622474670410156\n",
            "Epoch 181 loss: 50.91881561279297\n",
            "Epoch 182 loss: 49.27124786376953\n",
            "Epoch 183 loss: 47.677955627441406\n",
            "Epoch 184 loss: 46.13779067993164\n",
            "Epoch 185 loss: 44.647789001464844\n",
            "Epoch 186 loss: 43.20650863647461\n",
            "Epoch 187 loss: 41.812950134277344\n",
            "Epoch 188 loss: 40.465911865234375\n",
            "Epoch 189 loss: 39.163272857666016\n",
            "Epoch 190 loss: 37.902984619140625\n",
            "Epoch 191 loss: 36.68418884277344\n",
            "Epoch 192 loss: 35.505313873291016\n",
            "Epoch 193 loss: 34.36572265625\n",
            "Epoch 194 loss: 33.2635498046875\n",
            "Epoch 195 loss: 32.19769287109375\n",
            "Epoch 196 loss: 31.166057586669922\n",
            "Epoch 197 loss: 30.168649673461914\n",
            "Epoch 198 loss: 29.203290939331055\n",
            "Epoch 199 loss: 28.269529342651367\n",
            "Epoch 200 loss: 27.366422653198242\n",
            "Epoch 201 loss: 26.491798400878906\n",
            "Epoch 202 loss: 25.646522521972656\n",
            "Epoch 203 loss: 24.828784942626953\n",
            "Epoch 204 loss: 24.03746223449707\n",
            "Epoch 205 loss: 23.271760940551758\n",
            "Epoch 206 loss: 22.531200408935547\n",
            "Epoch 207 loss: 21.813871383666992\n",
            "Epoch 208 loss: 21.120206832885742\n",
            "Epoch 209 loss: 20.449478149414062\n",
            "Epoch 210 loss: 19.80084228515625\n",
            "Epoch 211 loss: 19.17268180847168\n",
            "Epoch 212 loss: 18.565162658691406\n",
            "Epoch 213 loss: 17.977388381958008\n",
            "Epoch 214 loss: 17.408742904663086\n",
            "Epoch 215 loss: 16.858381271362305\n",
            "Epoch 216 loss: 16.32512092590332\n",
            "Epoch 217 loss: 15.808923721313477\n",
            "Epoch 218 loss: 15.309853553771973\n",
            "Epoch 219 loss: 14.826777458190918\n",
            "Epoch 220 loss: 14.358613967895508\n",
            "Epoch 221 loss: 13.905811309814453\n",
            "Epoch 222 loss: 13.467766761779785\n",
            "Epoch 223 loss: 13.04367446899414\n",
            "Epoch 224 loss: 12.633417129516602\n",
            "Epoch 225 loss: 12.236652374267578\n",
            "Epoch 226 loss: 11.851995468139648\n",
            "Epoch 227 loss: 11.479477882385254\n",
            "Epoch 228 loss: 11.118911743164062\n",
            "Epoch 229 loss: 10.770406723022461\n",
            "Epoch 230 loss: 10.432971000671387\n",
            "Epoch 231 loss: 10.106240272521973\n",
            "Epoch 232 loss: 9.78976821899414\n",
            "Epoch 233 loss: 9.48321533203125\n",
            "Epoch 234 loss: 9.186996459960938\n",
            "Epoch 235 loss: 8.899937629699707\n",
            "Epoch 236 loss: 8.622004508972168\n",
            "Epoch 237 loss: 8.352978706359863\n",
            "Epoch 238 loss: 8.09255599975586\n",
            "Epoch 239 loss: 7.839931488037109\n",
            "Epoch 240 loss: 7.595770359039307\n",
            "Epoch 241 loss: 7.359030246734619\n",
            "Epoch 242 loss: 7.130112648010254\n",
            "Epoch 243 loss: 6.908260345458984\n",
            "Epoch 244 loss: 6.693466663360596\n",
            "Epoch 245 loss: 6.485328674316406\n",
            "Epoch 246 loss: 6.283690929412842\n",
            "Epoch 247 loss: 6.088550090789795\n",
            "Epoch 248 loss: 5.899674415588379\n",
            "Epoch 249 loss: 5.716547012329102\n",
            "Epoch 250 loss: 5.539107799530029\n",
            "Epoch 251 loss: 5.367434501647949\n",
            "Epoch 252 loss: 5.200899600982666\n",
            "Epoch 253 loss: 5.040071487426758\n",
            "Epoch 254 loss: 4.884332180023193\n",
            "Epoch 255 loss: 4.733293533325195\n",
            "Epoch 256 loss: 4.587110996246338\n",
            "Epoch 257 loss: 4.445279598236084\n",
            "Epoch 258 loss: 4.307955741882324\n",
            "Epoch 259 loss: 4.174905776977539\n",
            "Epoch 260 loss: 4.045968532562256\n",
            "Epoch 261 loss: 3.921229124069214\n",
            "Epoch 262 loss: 3.8004651069641113\n",
            "Epoch 263 loss: 3.6835224628448486\n",
            "Epoch 264 loss: 3.570021629333496\n",
            "Epoch 265 loss: 3.4600563049316406\n",
            "Epoch 266 loss: 3.3535995483398438\n",
            "Epoch 267 loss: 3.250511407852173\n",
            "Epoch 268 loss: 3.15071177482605\n",
            "Epoch 269 loss: 3.054067611694336\n",
            "Epoch 270 loss: 2.960451602935791\n",
            "Epoch 271 loss: 2.8695850372314453\n",
            "Epoch 272 loss: 2.781559944152832\n",
            "Epoch 273 loss: 2.696258544921875\n",
            "Epoch 274 loss: 2.6137163639068604\n",
            "Epoch 275 loss: 2.533622980117798\n",
            "Epoch 276 loss: 2.455875873565674\n",
            "Epoch 277 loss: 2.3806583881378174\n",
            "Epoch 278 loss: 2.3078160285949707\n",
            "Epoch 279 loss: 2.237337827682495\n",
            "Epoch 280 loss: 2.1693007946014404\n",
            "Epoch 281 loss: 2.102977991104126\n",
            "Epoch 282 loss: 2.038686990737915\n",
            "Epoch 283 loss: 1.976380467414856\n",
            "Epoch 284 loss: 1.9160548448562622\n",
            "Epoch 285 loss: 1.8574957847595215\n",
            "Epoch 286 loss: 1.8009512424468994\n",
            "Epoch 287 loss: 1.7458854913711548\n",
            "Epoch 288 loss: 1.6925876140594482\n",
            "Epoch 289 loss: 1.6409759521484375\n",
            "Epoch 290 loss: 1.5910487174987793\n",
            "Epoch 291 loss: 1.5426506996154785\n",
            "Epoch 292 loss: 1.4957466125488281\n",
            "Epoch 293 loss: 1.4500811100006104\n",
            "Epoch 294 loss: 1.406027913093567\n",
            "Epoch 295 loss: 1.3632243871688843\n",
            "Epoch 296 loss: 1.3217490911483765\n",
            "Epoch 297 loss: 1.2816052436828613\n",
            "Epoch 298 loss: 1.2427949905395508\n",
            "Epoch 299 loss: 1.2051507234573364\n",
            "Epoch 300 loss: 1.1684813499450684\n",
            "Epoch 301 loss: 1.1330280303955078\n",
            "Epoch 302 loss: 1.0986005067825317\n",
            "Epoch 303 loss: 1.065239667892456\n",
            "Epoch 304 loss: 1.0329513549804688\n",
            "Epoch 305 loss: 1.0016180276870728\n",
            "Epoch 306 loss: 0.9713387489318848\n",
            "Epoch 307 loss: 0.9419979453086853\n",
            "Epoch 308 loss: 0.9135154485702515\n",
            "Epoch 309 loss: 0.8859297037124634\n",
            "Epoch 310 loss: 0.8591628670692444\n",
            "Epoch 311 loss: 0.8332244157791138\n",
            "Epoch 312 loss: 0.8081772923469543\n",
            "Epoch 313 loss: 0.7837555408477783\n",
            "Epoch 314 loss: 0.760080873966217\n",
            "Epoch 315 loss: 0.7371622920036316\n",
            "Epoch 316 loss: 0.7150331139564514\n",
            "Epoch 317 loss: 0.6934699416160583\n",
            "Epoch 318 loss: 0.6725621819496155\n",
            "Epoch 319 loss: 0.6523441076278687\n",
            "Epoch 320 loss: 0.6327258944511414\n",
            "Epoch 321 loss: 0.6137418150901794\n",
            "Epoch 322 loss: 0.5952823162078857\n",
            "Epoch 323 loss: 0.5774524807929993\n",
            "Epoch 324 loss: 0.5601221323013306\n",
            "Epoch 325 loss: 0.5433257222175598\n",
            "Epoch 326 loss: 0.5270286798477173\n",
            "Epoch 327 loss: 0.5112415552139282\n",
            "Epoch 328 loss: 0.49586644768714905\n",
            "Epoch 329 loss: 0.4810435175895691\n",
            "Epoch 330 loss: 0.46657055616378784\n",
            "Epoch 331 loss: 0.4526265561580658\n",
            "Epoch 332 loss: 0.43907609581947327\n",
            "Epoch 333 loss: 0.4259506165981293\n",
            "Epoch 334 loss: 0.41314199566841125\n",
            "Epoch 335 loss: 0.4007413983345032\n",
            "Epoch 336 loss: 0.3887960910797119\n",
            "Epoch 337 loss: 0.37718144059181213\n",
            "Epoch 338 loss: 0.3658536672592163\n",
            "Epoch 339 loss: 0.35478949546813965\n",
            "Epoch 340 loss: 0.34409207105636597\n",
            "Epoch 341 loss: 0.33376994729042053\n",
            "Epoch 342 loss: 0.3237786293029785\n",
            "Epoch 343 loss: 0.3140588700771332\n",
            "Epoch 344 loss: 0.3046724796295166\n",
            "Epoch 345 loss: 0.29559436440467834\n",
            "Epoch 346 loss: 0.2867516279220581\n",
            "Epoch 347 loss: 0.27822017669677734\n",
            "Epoch 348 loss: 0.26992857456207275\n",
            "Epoch 349 loss: 0.2618560791015625\n",
            "Epoch 350 loss: 0.2540753185749054\n",
            "Epoch 351 loss: 0.2465330958366394\n",
            "Epoch 352 loss: 0.23916421830654144\n",
            "Epoch 353 loss: 0.23201003670692444\n",
            "Epoch 354 loss: 0.2250947654247284\n",
            "Epoch 355 loss: 0.21841245889663696\n",
            "Epoch 356 loss: 0.21195727586746216\n",
            "Epoch 357 loss: 0.20565427839756012\n",
            "Epoch 358 loss: 0.19958272576332092\n",
            "Epoch 359 loss: 0.19362901151180267\n",
            "Epoch 360 loss: 0.18784478306770325\n",
            "Epoch 361 loss: 0.1822655200958252\n",
            "Epoch 362 loss: 0.17688585817813873\n",
            "Epoch 363 loss: 0.17167527973651886\n",
            "Epoch 364 loss: 0.16662979125976562\n",
            "Epoch 365 loss: 0.16172091662883759\n",
            "Epoch 366 loss: 0.1569579541683197\n",
            "Epoch 367 loss: 0.15231382846832275\n",
            "Epoch 368 loss: 0.14783330261707306\n",
            "Epoch 369 loss: 0.14345434308052063\n",
            "Epoch 370 loss: 0.13920950889587402\n",
            "Epoch 371 loss: 0.13510695099830627\n",
            "Epoch 372 loss: 0.1310988962650299\n",
            "Epoch 373 loss: 0.12721648812294006\n",
            "Epoch 374 loss: 0.12345675379037857\n",
            "Epoch 375 loss: 0.11979567259550095\n",
            "Epoch 376 loss: 0.1162417083978653\n",
            "Epoch 377 loss: 0.11281299591064453\n",
            "Epoch 378 loss: 0.1094760000705719\n",
            "Epoch 379 loss: 0.10622888803482056\n",
            "Epoch 380 loss: 0.10306984186172485\n",
            "Epoch 381 loss: 0.10003568232059479\n",
            "Epoch 382 loss: 0.09710390120744705\n",
            "Epoch 383 loss: 0.09422509372234344\n",
            "Epoch 384 loss: 0.09142650663852692\n",
            "Epoch 385 loss: 0.0887155532836914\n",
            "Epoch 386 loss: 0.08610807359218597\n",
            "Epoch 387 loss: 0.08357477188110352\n",
            "Epoch 388 loss: 0.08111406117677689\n",
            "Epoch 389 loss: 0.07874148339033127\n",
            "Epoch 390 loss: 0.07640412449836731\n",
            "Epoch 391 loss: 0.07413521409034729\n",
            "Epoch 392 loss: 0.07194961607456207\n",
            "Epoch 393 loss: 0.06982896476984024\n",
            "Epoch 394 loss: 0.06776386499404907\n",
            "Epoch 395 loss: 0.06578455120325089\n",
            "Epoch 396 loss: 0.06386540830135345\n",
            "Epoch 397 loss: 0.06202027201652527\n",
            "Epoch 398 loss: 0.0601872056722641\n",
            "Epoch 399 loss: 0.058396387845277786\n",
            "Epoch 400 loss: 0.056676194071769714\n",
            "Epoch 401 loss: 0.05501750484108925\n",
            "Epoch 402 loss: 0.05340460315346718\n",
            "Epoch 403 loss: 0.051836539059877396\n",
            "Epoch 404 loss: 0.05031922087073326\n",
            "Epoch 405 loss: 0.04881769418716431\n",
            "Epoch 406 loss: 0.04737875983119011\n",
            "Epoch 407 loss: 0.04596789553761482\n",
            "Epoch 408 loss: 0.044623468071222305\n",
            "Epoch 409 loss: 0.04330534487962723\n",
            "Epoch 410 loss: 0.04202575236558914\n",
            "Epoch 411 loss: 0.04077150672674179\n",
            "Epoch 412 loss: 0.03955446928739548\n",
            "Epoch 413 loss: 0.03838576376438141\n",
            "Epoch 414 loss: 0.037234582006931305\n",
            "Epoch 415 loss: 0.03612412512302399\n",
            "Epoch 416 loss: 0.03504761680960655\n",
            "Epoch 417 loss: 0.03399864211678505\n",
            "Epoch 418 loss: 0.03300440311431885\n",
            "Epoch 419 loss: 0.032024919986724854\n",
            "Epoch 420 loss: 0.03104943409562111\n",
            "Epoch 421 loss: 0.030115509405732155\n",
            "Epoch 422 loss: 0.029211491346359253\n",
            "Epoch 423 loss: 0.02834693342447281\n",
            "Epoch 424 loss: 0.027510546147823334\n",
            "Epoch 425 loss: 0.02670164220035076\n",
            "Epoch 426 loss: 0.02591954544186592\n",
            "Epoch 427 loss: 0.025168435648083687\n",
            "Epoch 428 loss: 0.024428369477391243\n",
            "Epoch 429 loss: 0.02369934692978859\n",
            "Epoch 430 loss: 0.022999877110123634\n",
            "Epoch 431 loss: 0.02233368158340454\n",
            "Epoch 432 loss: 0.0216817706823349\n",
            "Epoch 433 loss: 0.021039515733718872\n",
            "Epoch 434 loss: 0.020415637642145157\n",
            "Epoch 435 loss: 0.019814033061265945\n",
            "Epoch 436 loss: 0.01922142505645752\n",
            "Epoch 437 loss: 0.018654484301805496\n",
            "Epoch 438 loss: 0.018108347430825233\n",
            "Epoch 439 loss: 0.01757436990737915\n",
            "Epoch 440 loss: 0.017052367329597473\n",
            "Epoch 441 loss: 0.016553938388824463\n",
            "Epoch 442 loss: 0.016070637851953506\n",
            "Epoch 443 loss: 0.015590686351060867\n",
            "Epoch 444 loss: 0.0151330241933465\n",
            "Epoch 445 loss: 0.014704374596476555\n",
            "Epoch 446 loss: 0.014285530894994736\n",
            "Epoch 447 loss: 0.0138799287378788\n",
            "Epoch 448 loss: 0.013487255200743675\n",
            "Epoch 449 loss: 0.013100216165184975\n",
            "Epoch 450 loss: 0.012715370394289494\n",
            "Epoch 451 loss: 0.012332872487604618\n",
            "Epoch 452 loss: 0.011962890625\n",
            "Epoch 453 loss: 0.011618271470069885\n",
            "Epoch 454 loss: 0.01127544790506363\n",
            "Epoch 455 loss: 0.010940950363874435\n",
            "Epoch 456 loss: 0.010614633560180664\n",
            "Epoch 457 loss: 0.010302547365427017\n",
            "Epoch 458 loss: 0.009989016689360142\n",
            "Epoch 459 loss: 0.009689340367913246\n",
            "Epoch 460 loss: 0.009400145150721073\n",
            "Epoch 461 loss: 0.009121159091591835\n",
            "Epoch 462 loss: 0.008852117694914341\n",
            "Epoch 463 loss: 0.008587103337049484\n",
            "Epoch 464 loss: 0.008342833258211613\n",
            "Epoch 465 loss: 0.008102087303996086\n",
            "Epoch 466 loss: 0.007875695824623108\n",
            "Epoch 467 loss: 0.0076471734791994095\n",
            "Epoch 468 loss: 0.00742727518081665\n",
            "Epoch 469 loss: 0.007215768098831177\n",
            "Epoch 470 loss: 0.007012426853179932\n",
            "Epoch 471 loss: 0.006814510561525822\n",
            "Epoch 472 loss: 0.006616944447159767\n",
            "Epoch 473 loss: 0.0064222849905490875\n",
            "Epoch 474 loss: 0.0062329405918717384\n",
            "Epoch 475 loss: 0.006051176227629185\n",
            "Epoch 476 loss: 0.005876779556274414\n",
            "Epoch 477 loss: 0.005707237869501114\n",
            "Epoch 478 loss: 0.005544722080230713\n",
            "Epoch 479 loss: 0.005380075424909592\n",
            "Epoch 480 loss: 0.005217910744249821\n",
            "Epoch 481 loss: 0.005058227106928825\n",
            "Epoch 482 loss: 0.004907436668872833\n",
            "Epoch 483 loss: 0.004761033691465855\n",
            "Epoch 484 loss: 0.004616847261786461\n",
            "Epoch 485 loss: 0.004476919770240784\n",
            "Epoch 486 loss: 0.004341156221926212\n",
            "Epoch 487 loss: 0.004209462553262711\n",
            "Epoch 488 loss: 0.004087598063051701\n",
            "Epoch 489 loss: 0.003965601325035095\n",
            "Epoch 490 loss: 0.0038454532623291016\n",
            "Epoch 491 loss: 0.0037308803293854\n",
            "Epoch 492 loss: 0.003621712327003479\n",
            "Epoch 493 loss: 0.0035123564302921295\n",
            "Epoch 494 loss: 0.0034064578358083963\n",
            "Epoch 495 loss: 0.003303934121504426\n",
            "Epoch 496 loss: 0.003201249986886978\n",
            "Epoch 497 loss: 0.003103586146607995\n",
            "Epoch 498 loss: 0.0030107833445072174\n",
            "Epoch 499 loss: 0.0029226879123598337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFAacdy46bX"
      },
      "source": [
        "3.2.2 Решить задачу 3.1.2, воспользовавшись оптимизатором `optim.Adam` для применения пакетного градиентого спуска. Вывести график функции потерь в зависимости от номера эпохи. Вывести на одном графике исходные данные и предсказанные значения."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(torch.linspace(0, 1, 100).view(-1, 1))\n",
        "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) "
      ],
      "metadata": {
        "id": "4I0BGYq6zCb6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def forward(self, inputs):\n",
        "        inputs[inputs < 0] = 0\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "sv2SlnUdzFqo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.loss = ((y_pred - y_true)**2).mean()\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        y_pred = torch.autograd.Variable(y_pred, requires_grad=True)\n",
        "        y_true = torch.autograd.Variable(y_true, requires_grad=False)\n",
        "        (((y_pred - y_true)**2).mean()).backward()\n",
        "        self.dinput = y_pred.grad"
      ],
      "metadata": {
        "id": "vtG0luSZzHTb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    def __init__(self, n_features, n_neurons):\n",
        "        self.weights = torch.nn.Parameter(torch.randn(n_features, n_neurons), requires_grad=True) \n",
        "        self.biases = torch.nn.Parameter(torch.randn(n_neurons), requires_grad=True)\n",
        "      \n",
        "    def parameters(self):\n",
        "        yield self.weights\n",
        "        yield self.biases\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        return torch.matmul(inputs, self.weights) + self.biases\n",
        "    \n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = torch.matmul(self.inputs.T, dvalues)\n",
        "        self.dbiases = torch.sum(dvalues, axis=0)\n",
        "        self.dinputs = torch.matmul(dvalues, self.weights.T)\n",
        "        return self.dweights, self.dbiases"
      ],
      "metadata": {
        "id": "tKIv5lkTzJEU"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron_1 = Linear(1, 100)\n",
        "neuron_2 = Linear(100, 1)\n",
        "loss = MSELoss()\n",
        "activation = ReLU()\n",
        "optimizer = optim.Adam([{'params': neuron_1.parameters()}, {'params': neuron_2.parameters()}], lr=0.01)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(500):\n",
        "    # forward pass\n",
        "    y_pred = neuron_2.forward(activation.forward(neuron_1.forward(X)))\n",
        "    # y_pred = neuron_2.forward(neuron_1.forward(X))\n",
        "    curr_loss = loss.forward(y_pred, y)\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.forward(y_pred, y).backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch} loss: {curr_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gr9zMx4zLlS",
        "outputId": "41ddf16c-9f03-42d8-cdf3-e2b2ded54cf4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 510.2767639160156\n",
            "Epoch 1 loss: 455.5447998046875\n",
            "Epoch 2 loss: 404.377197265625\n",
            "Epoch 3 loss: 356.7664489746094\n",
            "Epoch 4 loss: 312.6713562011719\n",
            "Epoch 5 loss: 272.040283203125\n",
            "Epoch 6 loss: 234.8047637939453\n",
            "Epoch 7 loss: 200.88461303710938\n",
            "Epoch 8 loss: 170.19789123535156\n",
            "Epoch 9 loss: 142.63714599609375\n",
            "Epoch 10 loss: 118.09918975830078\n",
            "Epoch 11 loss: 96.45463562011719\n",
            "Epoch 12 loss: 77.57599639892578\n",
            "Epoch 13 loss: 61.316410064697266\n",
            "Epoch 14 loss: 47.52101516723633\n",
            "Epoch 15 loss: 36.026580810546875\n",
            "Epoch 16 loss: 26.656078338623047\n",
            "Epoch 17 loss: 19.22919464111328\n",
            "Epoch 18 loss: 13.555279731750488\n",
            "Epoch 19 loss: 9.437031745910645\n",
            "Epoch 20 loss: 6.671666145324707\n",
            "Epoch 21 loss: 5.054286956787109\n",
            "Epoch 22 loss: 4.380874156951904\n",
            "Epoch 23 loss: 4.451577663421631\n",
            "Epoch 24 loss: 5.074786186218262\n",
            "Epoch 25 loss: 6.071114540100098\n",
            "Epoch 26 loss: 7.278163433074951\n",
            "Epoch 27 loss: 8.553799629211426\n",
            "Epoch 28 loss: 9.778411865234375\n",
            "Epoch 29 loss: 10.857965469360352\n",
            "Epoch 30 loss: 11.723073959350586\n",
            "Epoch 31 loss: 12.329477310180664\n",
            "Epoch 32 loss: 12.655725479125977\n",
            "Epoch 33 loss: 12.701523780822754\n",
            "Epoch 34 loss: 12.48234748840332\n",
            "Epoch 35 loss: 12.027034759521484\n",
            "Epoch 36 loss: 11.374263763427734\n",
            "Epoch 37 loss: 10.567745208740234\n",
            "Epoch 38 loss: 9.6536283493042\n",
            "Epoch 39 loss: 8.676923751831055\n",
            "Epoch 40 loss: 7.6800856590271\n",
            "Epoch 41 loss: 6.700560092926025\n",
            "Epoch 42 loss: 5.769391059875488\n",
            "Epoch 43 loss: 4.911153793334961\n",
            "Epoch 44 loss: 4.143673419952393\n",
            "Epoch 45 loss: 3.4778225421905518\n",
            "Epoch 46 loss: 2.918341875076294\n",
            "Epoch 47 loss: 2.464539051055908\n",
            "Epoch 48 loss: 2.11112642288208\n",
            "Epoch 49 loss: 1.849202275276184\n",
            "Epoch 50 loss: 1.6670461893081665\n",
            "Epoch 51 loss: 1.5515332221984863\n",
            "Epoch 52 loss: 1.4888290166854858\n",
            "Epoch 53 loss: 1.4652395248413086\n",
            "Epoch 54 loss: 1.4677122831344604\n",
            "Epoch 55 loss: 1.484498143196106\n",
            "Epoch 56 loss: 1.5055456161499023\n",
            "Epoch 57 loss: 1.5226967334747314\n",
            "Epoch 58 loss: 1.529841661453247\n",
            "Epoch 59 loss: 1.5228281021118164\n",
            "Epoch 60 loss: 1.4993727207183838\n",
            "Epoch 61 loss: 1.4590100049972534\n",
            "Epoch 62 loss: 1.4026305675506592\n",
            "Epoch 63 loss: 1.3321194648742676\n",
            "Epoch 64 loss: 1.2505502700805664\n",
            "Epoch 65 loss: 1.1612070798873901\n",
            "Epoch 66 loss: 1.0678473711013794\n",
            "Epoch 67 loss: 0.9739090800285339\n",
            "Epoch 68 loss: 0.8828787207603455\n",
            "Epoch 69 loss: 0.7975834012031555\n",
            "Epoch 70 loss: 0.7202694416046143\n",
            "Epoch 71 loss: 0.65264493227005\n",
            "Epoch 72 loss: 0.5956130027770996\n",
            "Epoch 73 loss: 0.5494874715805054\n",
            "Epoch 74 loss: 0.5139196515083313\n",
            "Epoch 75 loss: 0.4880429208278656\n",
            "Epoch 76 loss: 0.47059309482574463\n",
            "Epoch 77 loss: 0.46006354689598083\n",
            "Epoch 78 loss: 0.4547080993652344\n",
            "Epoch 79 loss: 0.45282113552093506\n",
            "Epoch 80 loss: 0.452820360660553\n",
            "Epoch 81 loss: 0.45328861474990845\n",
            "Epoch 82 loss: 0.4531005024909973\n",
            "Epoch 83 loss: 0.45140784978866577\n",
            "Epoch 84 loss: 0.44776439666748047\n",
            "Epoch 85 loss: 0.4419916570186615\n",
            "Epoch 86 loss: 0.43419358134269714\n",
            "Epoch 87 loss: 0.4246867299079895\n",
            "Epoch 88 loss: 0.41390717029571533\n",
            "Epoch 89 loss: 0.4023820459842682\n",
            "Epoch 90 loss: 0.39066779613494873\n",
            "Epoch 91 loss: 0.37925660610198975\n",
            "Epoch 92 loss: 0.3685859441757202\n",
            "Epoch 93 loss: 0.358987420797348\n",
            "Epoch 94 loss: 0.3506419062614441\n",
            "Epoch 95 loss: 0.343644380569458\n",
            "Epoch 96 loss: 0.3379511535167694\n",
            "Epoch 97 loss: 0.3334771692752838\n",
            "Epoch 98 loss: 0.33000946044921875\n",
            "Epoch 99 loss: 0.3273564577102661\n",
            "Epoch 100 loss: 0.325242817401886\n",
            "Epoch 101 loss: 0.3234763741493225\n",
            "Epoch 102 loss: 0.3218322992324829\n",
            "Epoch 103 loss: 0.3201198875904083\n",
            "Epoch 104 loss: 0.3182319104671478\n",
            "Epoch 105 loss: 0.31606587767601013\n",
            "Epoch 106 loss: 0.31357496976852417\n",
            "Epoch 107 loss: 0.31078609824180603\n",
            "Epoch 108 loss: 0.30772218108177185\n",
            "Epoch 109 loss: 0.304411917924881\n",
            "Epoch 110 loss: 0.3009544610977173\n",
            "Epoch 111 loss: 0.29742786288261414\n",
            "Epoch 112 loss: 0.2938992381095886\n",
            "Epoch 113 loss: 0.2904109060764313\n",
            "Epoch 114 loss: 0.28702428936958313\n",
            "Epoch 115 loss: 0.2837768495082855\n",
            "Epoch 116 loss: 0.28068679571151733\n",
            "Epoch 117 loss: 0.2777480185031891\n",
            "Epoch 118 loss: 0.2749364674091339\n",
            "Epoch 119 loss: 0.27223873138427734\n",
            "Epoch 120 loss: 0.2696378231048584\n",
            "Epoch 121 loss: 0.2670944035053253\n",
            "Epoch 122 loss: 0.2645939588546753\n",
            "Epoch 123 loss: 0.26212117075920105\n",
            "Epoch 124 loss: 0.25964435935020447\n",
            "Epoch 125 loss: 0.2571517825126648\n",
            "Epoch 126 loss: 0.25463780760765076\n",
            "Epoch 127 loss: 0.2521016597747803\n",
            "Epoch 128 loss: 0.24954089522361755\n",
            "Epoch 129 loss: 0.2469586879014969\n",
            "Epoch 130 loss: 0.24438320100307465\n",
            "Epoch 131 loss: 0.24181979894638062\n",
            "Epoch 132 loss: 0.2392783761024475\n",
            "Epoch 133 loss: 0.23676982522010803\n",
            "Epoch 134 loss: 0.2343023270368576\n",
            "Epoch 135 loss: 0.23187962174415588\n",
            "Epoch 136 loss: 0.22950437664985657\n",
            "Epoch 137 loss: 0.22718681395053864\n",
            "Epoch 138 loss: 0.22492444515228271\n",
            "Epoch 139 loss: 0.22269928455352783\n",
            "Epoch 140 loss: 0.22049960494041443\n",
            "Epoch 141 loss: 0.2183436006307602\n",
            "Epoch 142 loss: 0.2162161022424698\n",
            "Epoch 143 loss: 0.2141132801771164\n",
            "Epoch 144 loss: 0.21204990148544312\n",
            "Epoch 145 loss: 0.2100135236978531\n",
            "Epoch 146 loss: 0.20799663662910461\n",
            "Epoch 147 loss: 0.20599880814552307\n",
            "Epoch 148 loss: 0.20403017103672028\n",
            "Epoch 149 loss: 0.20206411182880402\n",
            "Epoch 150 loss: 0.20011746883392334\n",
            "Epoch 151 loss: 0.19819152355194092\n",
            "Epoch 152 loss: 0.196293905377388\n",
            "Epoch 153 loss: 0.19443383812904358\n",
            "Epoch 154 loss: 0.19260093569755554\n",
            "Epoch 155 loss: 0.19079215824604034\n",
            "Epoch 156 loss: 0.18899640440940857\n",
            "Epoch 157 loss: 0.187215194106102\n",
            "Epoch 158 loss: 0.185468852519989\n",
            "Epoch 159 loss: 0.1837480515241623\n",
            "Epoch 160 loss: 0.1820472925901413\n",
            "Epoch 161 loss: 0.18036577105522156\n",
            "Epoch 162 loss: 0.1787094920873642\n",
            "Epoch 163 loss: 0.1770814061164856\n",
            "Epoch 164 loss: 0.17545422911643982\n",
            "Epoch 165 loss: 0.17384269833564758\n",
            "Epoch 166 loss: 0.17224663496017456\n",
            "Epoch 167 loss: 0.17066390812397003\n",
            "Epoch 168 loss: 0.16910022497177124\n",
            "Epoch 169 loss: 0.16756515204906464\n",
            "Epoch 170 loss: 0.16604632139205933\n",
            "Epoch 171 loss: 0.16452538967132568\n",
            "Epoch 172 loss: 0.16301913559436798\n",
            "Epoch 173 loss: 0.16152796149253845\n",
            "Epoch 174 loss: 0.1600518375635147\n",
            "Epoch 175 loss: 0.1585911363363266\n",
            "Epoch 176 loss: 0.15715797245502472\n",
            "Epoch 177 loss: 0.1557275652885437\n",
            "Epoch 178 loss: 0.15429890155792236\n",
            "Epoch 179 loss: 0.1528831571340561\n",
            "Epoch 180 loss: 0.15148982405662537\n",
            "Epoch 181 loss: 0.15011504292488098\n",
            "Epoch 182 loss: 0.14875441789627075\n",
            "Epoch 183 loss: 0.14739461243152618\n",
            "Epoch 184 loss: 0.14603938162326813\n",
            "Epoch 185 loss: 0.1446998566389084\n",
            "Epoch 186 loss: 0.14337271451950073\n",
            "Epoch 187 loss: 0.14205817878246307\n",
            "Epoch 188 loss: 0.1407558023929596\n",
            "Epoch 189 loss: 0.13944587111473083\n",
            "Epoch 190 loss: 0.13814733922481537\n",
            "Epoch 191 loss: 0.1368640661239624\n",
            "Epoch 192 loss: 0.13559220731258392\n",
            "Epoch 193 loss: 0.13433223962783813\n",
            "Epoch 194 loss: 0.13308219611644745\n",
            "Epoch 195 loss: 0.13183507323265076\n",
            "Epoch 196 loss: 0.1305999457836151\n",
            "Epoch 197 loss: 0.12937553226947784\n",
            "Epoch 198 loss: 0.12816236913204193\n",
            "Epoch 199 loss: 0.1269480437040329\n",
            "Epoch 200 loss: 0.12573625147342682\n",
            "Epoch 201 loss: 0.12453378736972809\n",
            "Epoch 202 loss: 0.12334156036376953\n",
            "Epoch 203 loss: 0.12215950340032578\n",
            "Epoch 204 loss: 0.12097284942865372\n",
            "Epoch 205 loss: 0.11978810280561447\n",
            "Epoch 206 loss: 0.11861974000930786\n",
            "Epoch 207 loss: 0.1174616813659668\n",
            "Epoch 208 loss: 0.11631643027067184\n",
            "Epoch 209 loss: 0.11516130715608597\n",
            "Epoch 210 loss: 0.11401844769716263\n",
            "Epoch 211 loss: 0.11288880556821823\n",
            "Epoch 212 loss: 0.11177651584148407\n",
            "Epoch 213 loss: 0.11066434532403946\n",
            "Epoch 214 loss: 0.10955697298049927\n",
            "Epoch 215 loss: 0.10845921188592911\n",
            "Epoch 216 loss: 0.10737118870019913\n",
            "Epoch 217 loss: 0.10628383606672287\n",
            "Epoch 218 loss: 0.10519558191299438\n",
            "Epoch 219 loss: 0.10411801934242249\n",
            "Epoch 220 loss: 0.10305247455835342\n",
            "Epoch 221 loss: 0.10199584066867828\n",
            "Epoch 222 loss: 0.100943423807621\n",
            "Epoch 223 loss: 0.0999002680182457\n",
            "Epoch 224 loss: 0.09886657446622849\n",
            "Epoch 225 loss: 0.09783860296010971\n",
            "Epoch 226 loss: 0.0968167707324028\n",
            "Epoch 227 loss: 0.09580962359905243\n",
            "Epoch 228 loss: 0.09481681138277054\n",
            "Epoch 229 loss: 0.09383703023195267\n",
            "Epoch 230 loss: 0.09286985546350479\n",
            "Epoch 231 loss: 0.09191298484802246\n",
            "Epoch 232 loss: 0.09096640348434448\n",
            "Epoch 233 loss: 0.09003260731697083\n",
            "Epoch 234 loss: 0.0891098603606224\n",
            "Epoch 235 loss: 0.08819739520549774\n",
            "Epoch 236 loss: 0.08729516714811325\n",
            "Epoch 237 loss: 0.08641211688518524\n",
            "Epoch 238 loss: 0.0855395570397377\n",
            "Epoch 239 loss: 0.08467742800712585\n",
            "Epoch 240 loss: 0.0838291198015213\n",
            "Epoch 241 loss: 0.08300422877073288\n",
            "Epoch 242 loss: 0.0821898877620697\n",
            "Epoch 243 loss: 0.08138592541217804\n",
            "Epoch 244 loss: 0.08059941977262497\n",
            "Epoch 245 loss: 0.0798344686627388\n",
            "Epoch 246 loss: 0.07908016443252563\n",
            "Epoch 247 loss: 0.0783407986164093\n",
            "Epoch 248 loss: 0.0776253268122673\n",
            "Epoch 249 loss: 0.07693067938089371\n",
            "Epoch 250 loss: 0.07624673843383789\n",
            "Epoch 251 loss: 0.07557309418916702\n",
            "Epoch 252 loss: 0.07492149621248245\n",
            "Epoch 253 loss: 0.07429452240467072\n",
            "Epoch 254 loss: 0.07368119806051254\n",
            "Epoch 255 loss: 0.0730779767036438\n",
            "Epoch 256 loss: 0.07249411195516586\n",
            "Epoch 257 loss: 0.07194571197032928\n",
            "Epoch 258 loss: 0.07140752673149109\n",
            "Epoch 259 loss: 0.07088147848844528\n",
            "Epoch 260 loss: 0.07036492228507996\n",
            "Epoch 261 loss: 0.06988129019737244\n",
            "Epoch 262 loss: 0.06941623985767365\n",
            "Epoch 263 loss: 0.06896038353443146\n",
            "Epoch 264 loss: 0.06851308792829514\n",
            "Epoch 265 loss: 0.06807403266429901\n",
            "Epoch 266 loss: 0.06766253709793091\n",
            "Epoch 267 loss: 0.0672699362039566\n",
            "Epoch 268 loss: 0.06688510626554489\n",
            "Epoch 269 loss: 0.06650765240192413\n",
            "Epoch 270 loss: 0.06613720208406448\n",
            "Epoch 271 loss: 0.06577333062887192\n",
            "Epoch 272 loss: 0.06543461233377457\n",
            "Epoch 273 loss: 0.0651145800948143\n",
            "Epoch 274 loss: 0.0648021250963211\n",
            "Epoch 275 loss: 0.0644959807395935\n",
            "Epoch 276 loss: 0.06419721245765686\n",
            "Epoch 277 loss: 0.06390392035245895\n",
            "Epoch 278 loss: 0.06361592561006546\n",
            "Epoch 279 loss: 0.0633329227566719\n",
            "Epoch 280 loss: 0.06307189911603928\n",
            "Epoch 281 loss: 0.06282138079404831\n",
            "Epoch 282 loss: 0.06257500499486923\n",
            "Epoch 283 loss: 0.062334585934877396\n",
            "Epoch 284 loss: 0.06209906190633774\n",
            "Epoch 285 loss: 0.06186750531196594\n",
            "Epoch 286 loss: 0.06163991615176201\n",
            "Epoch 287 loss: 0.061416178941726685\n",
            "Epoch 288 loss: 0.06119605898857117\n",
            "Epoch 289 loss: 0.06097916141152382\n",
            "Epoch 290 loss: 0.06076545640826225\n",
            "Epoch 291 loss: 0.06055634841322899\n",
            "Epoch 292 loss: 0.06035139039158821\n",
            "Epoch 293 loss: 0.06014953553676605\n",
            "Epoch 294 loss: 0.059953637421131134\n",
            "Epoch 295 loss: 0.05976768955588341\n",
            "Epoch 296 loss: 0.05958367511630058\n",
            "Epoch 297 loss: 0.05940176919102669\n",
            "Epoch 298 loss: 0.05922187864780426\n",
            "Epoch 299 loss: 0.059044018387794495\n",
            "Epoch 300 loss: 0.05886828526854515\n",
            "Epoch 301 loss: 0.05869461968541145\n",
            "Epoch 302 loss: 0.058523040264844894\n",
            "Epoch 303 loss: 0.058353520929813385\n",
            "Epoch 304 loss: 0.058185964822769165\n",
            "Epoch 305 loss: 0.058020636439323425\n",
            "Epoch 306 loss: 0.05785732716321945\n",
            "Epoch 307 loss: 0.05769604071974754\n",
            "Epoch 308 loss: 0.057536590844392776\n",
            "Epoch 309 loss: 0.05737893283367157\n",
            "Epoch 310 loss: 0.057223182171583176\n",
            "Epoch 311 loss: 0.057069193571805954\n",
            "Epoch 312 loss: 0.05691705271601677\n",
            "Epoch 313 loss: 0.05676746368408203\n",
            "Epoch 314 loss: 0.05662006884813309\n",
            "Epoch 315 loss: 0.05647430568933487\n",
            "Epoch 316 loss: 0.05633028596639633\n",
            "Epoch 317 loss: 0.05618813633918762\n",
            "Epoch 318 loss: 0.05604773387312889\n",
            "Epoch 319 loss: 0.0559089370071888\n",
            "Epoch 320 loss: 0.0557718388736248\n",
            "Epoch 321 loss: 0.055636316537857056\n",
            "Epoch 322 loss: 0.05550239980220795\n",
            "Epoch 323 loss: 0.05537009239196777\n",
            "Epoch 324 loss: 0.055239319801330566\n",
            "Epoch 325 loss: 0.055110443383455276\n",
            "Epoch 326 loss: 0.054983075708150864\n",
            "Epoch 327 loss: 0.05485724285244942\n",
            "Epoch 328 loss: 0.05473434552550316\n",
            "Epoch 329 loss: 0.054613545536994934\n",
            "Epoch 330 loss: 0.05449332669377327\n",
            "Epoch 331 loss: 0.05437382683157921\n",
            "Epoch 332 loss: 0.05425507575273514\n",
            "Epoch 333 loss: 0.0541376955807209\n",
            "Epoch 334 loss: 0.05402333289384842\n",
            "Epoch 335 loss: 0.05391022190451622\n",
            "Epoch 336 loss: 0.05379828065633774\n",
            "Epoch 337 loss: 0.05368853732943535\n",
            "Epoch 338 loss: 0.053580574691295624\n",
            "Epoch 339 loss: 0.05347365885972977\n",
            "Epoch 340 loss: 0.05336743965744972\n",
            "Epoch 341 loss: 0.053263261914253235\n",
            "Epoch 342 loss: 0.05316118896007538\n",
            "Epoch 343 loss: 0.053059566766023636\n",
            "Epoch 344 loss: 0.05295969918370247\n",
            "Epoch 345 loss: 0.05286087095737457\n",
            "Epoch 346 loss: 0.05276311933994293\n",
            "Epoch 347 loss: 0.05266513675451279\n",
            "Epoch 348 loss: 0.05256854370236397\n",
            "Epoch 349 loss: 0.05247284844517708\n",
            "Epoch 350 loss: 0.05237855017185211\n",
            "Epoch 351 loss: 0.0522845983505249\n",
            "Epoch 352 loss: 0.05219199135899544\n",
            "Epoch 353 loss: 0.05210021510720253\n",
            "Epoch 354 loss: 0.05200951173901558\n",
            "Epoch 355 loss: 0.051919687539339066\n",
            "Epoch 356 loss: 0.05183081701397896\n",
            "Epoch 357 loss: 0.05174272134900093\n",
            "Epoch 358 loss: 0.05165548622608185\n",
            "Epoch 359 loss: 0.051569484174251556\n",
            "Epoch 360 loss: 0.051483798772096634\n",
            "Epoch 361 loss: 0.051399633288383484\n",
            "Epoch 362 loss: 0.051316212862730026\n",
            "Epoch 363 loss: 0.051233530044555664\n",
            "Epoch 364 loss: 0.051151543855667114\n",
            "Epoch 365 loss: 0.05107063427567482\n",
            "Epoch 366 loss: 0.05099041014909744\n",
            "Epoch 367 loss: 0.05091094598174095\n",
            "Epoch 368 loss: 0.05083232372999191\n",
            "Epoch 369 loss: 0.050755858421325684\n",
            "Epoch 370 loss: 0.05067925527691841\n",
            "Epoch 371 loss: 0.0506025031208992\n",
            "Epoch 372 loss: 0.050527364015579224\n",
            "Epoch 373 loss: 0.05045337229967117\n",
            "Epoch 374 loss: 0.05037993937730789\n",
            "Epoch 375 loss: 0.05030720308423042\n",
            "Epoch 376 loss: 0.05023514851927757\n",
            "Epoch 377 loss: 0.050163645297288895\n",
            "Epoch 378 loss: 0.05009282007813454\n",
            "Epoch 379 loss: 0.05002273991703987\n",
            "Epoch 380 loss: 0.04995318874716759\n",
            "Epoch 381 loss: 0.049884382635354996\n",
            "Epoch 382 loss: 0.04981618747115135\n",
            "Epoch 383 loss: 0.049749039113521576\n",
            "Epoch 384 loss: 0.0496826171875\n",
            "Epoch 385 loss: 0.04961690306663513\n",
            "Epoch 386 loss: 0.04955172911286354\n",
            "Epoch 387 loss: 0.04948706924915314\n",
            "Epoch 388 loss: 0.04942343384027481\n",
            "Epoch 389 loss: 0.049360428005456924\n",
            "Epoch 390 loss: 0.049297865480184555\n",
            "Epoch 391 loss: 0.04923684149980545\n",
            "Epoch 392 loss: 0.049175962805747986\n",
            "Epoch 393 loss: 0.0491156205534935\n",
            "Epoch 394 loss: 0.04905626177787781\n",
            "Epoch 395 loss: 0.048997387290000916\n",
            "Epoch 396 loss: 0.04893897473812103\n",
            "Epoch 397 loss: 0.048880916088819504\n",
            "Epoch 398 loss: 0.04882337525486946\n",
            "Epoch 399 loss: 0.048766303807497025\n",
            "Epoch 400 loss: 0.048710357397794724\n",
            "Epoch 401 loss: 0.04865436628460884\n",
            "Epoch 402 loss: 0.0485987514257431\n",
            "Epoch 403 loss: 0.048544250428676605\n",
            "Epoch 404 loss: 0.04848991706967354\n",
            "Epoch 405 loss: 0.04843590781092644\n",
            "Epoch 406 loss: 0.048382438719272614\n",
            "Epoch 407 loss: 0.04832937568426132\n",
            "Epoch 408 loss: 0.04827669635415077\n",
            "Epoch 409 loss: 0.0482245571911335\n",
            "Epoch 410 loss: 0.04817274957895279\n",
            "Epoch 411 loss: 0.04812148958444595\n",
            "Epoch 412 loss: 0.04807073622941971\n",
            "Epoch 413 loss: 0.04802039638161659\n",
            "Epoch 414 loss: 0.04797042906284332\n",
            "Epoch 415 loss: 0.04792097210884094\n",
            "Epoch 416 loss: 0.04787170886993408\n",
            "Epoch 417 loss: 0.04782282933592796\n",
            "Epoch 418 loss: 0.047774240374565125\n",
            "Epoch 419 loss: 0.04772602394223213\n",
            "Epoch 420 loss: 0.047678135335445404\n",
            "Epoch 421 loss: 0.04763069003820419\n",
            "Epoch 422 loss: 0.04758354276418686\n",
            "Epoch 423 loss: 0.04753674566745758\n",
            "Epoch 424 loss: 0.047490425407886505\n",
            "Epoch 425 loss: 0.047443609684705734\n",
            "Epoch 426 loss: 0.04739652946591377\n",
            "Epoch 427 loss: 0.047349583357572556\n",
            "Epoch 428 loss: 0.04730280488729477\n",
            "Epoch 429 loss: 0.04725624620914459\n",
            "Epoch 430 loss: 0.04721067100763321\n",
            "Epoch 431 loss: 0.04716435819864273\n",
            "Epoch 432 loss: 0.047118499875068665\n",
            "Epoch 433 loss: 0.047073256224393845\n",
            "Epoch 434 loss: 0.04702802747488022\n",
            "Epoch 435 loss: 0.046982623636722565\n",
            "Epoch 436 loss: 0.04693751037120819\n",
            "Epoch 437 loss: 0.04689228907227516\n",
            "Epoch 438 loss: 0.04684732109308243\n",
            "Epoch 439 loss: 0.04680246859788895\n",
            "Epoch 440 loss: 0.046757787466049194\n",
            "Epoch 441 loss: 0.04671328142285347\n",
            "Epoch 442 loss: 0.046670109033584595\n",
            "Epoch 443 loss: 0.04662624001502991\n",
            "Epoch 444 loss: 0.04658176004886627\n",
            "Epoch 445 loss: 0.04653853550553322\n",
            "Epoch 446 loss: 0.046495452523231506\n",
            "Epoch 447 loss: 0.046452559530735016\n",
            "Epoch 448 loss: 0.04640969634056091\n",
            "Epoch 449 loss: 0.0463666096329689\n",
            "Epoch 450 loss: 0.04632365703582764\n",
            "Epoch 451 loss: 0.046280842274427414\n",
            "Epoch 452 loss: 0.04623845964670181\n",
            "Epoch 453 loss: 0.04619612172245979\n",
            "Epoch 454 loss: 0.04615413025021553\n",
            "Epoch 455 loss: 0.04611222445964813\n",
            "Epoch 456 loss: 0.04607076942920685\n",
            "Epoch 457 loss: 0.04602925479412079\n",
            "Epoch 458 loss: 0.04598809406161308\n",
            "Epoch 459 loss: 0.045947227627038956\n",
            "Epoch 460 loss: 0.04590625315904617\n",
            "Epoch 461 loss: 0.04586547240614891\n",
            "Epoch 462 loss: 0.04582495614886284\n",
            "Epoch 463 loss: 0.04578515887260437\n",
            "Epoch 464 loss: 0.0457446351647377\n",
            "Epoch 465 loss: 0.045705005526542664\n",
            "Epoch 466 loss: 0.045665524899959564\n",
            "Epoch 467 loss: 0.04562629759311676\n",
            "Epoch 468 loss: 0.045587025582790375\n",
            "Epoch 469 loss: 0.045547932386398315\n",
            "Epoch 470 loss: 0.04550895839929581\n",
            "Epoch 471 loss: 0.04547009989619255\n",
            "Epoch 472 loss: 0.04543139785528183\n",
            "Epoch 473 loss: 0.04539285600185394\n",
            "Epoch 474 loss: 0.045354556292295456\n",
            "Epoch 475 loss: 0.045316293835639954\n",
            "Epoch 476 loss: 0.045279622077941895\n",
            "Epoch 477 loss: 0.045241788029670715\n",
            "Epoch 478 loss: 0.045203372836112976\n",
            "Epoch 479 loss: 0.04516621679067612\n",
            "Epoch 480 loss: 0.04512919485569\n",
            "Epoch 481 loss: 0.04509221017360687\n",
            "Epoch 482 loss: 0.04505498707294464\n",
            "Epoch 483 loss: 0.04501784220337868\n",
            "Epoch 484 loss: 0.04498079791665077\n",
            "Epoch 485 loss: 0.04494376480579376\n",
            "Epoch 486 loss: 0.04490683972835541\n",
            "Epoch 487 loss: 0.044871047139167786\n",
            "Epoch 488 loss: 0.044834256172180176\n",
            "Epoch 489 loss: 0.044797588139772415\n",
            "Epoch 490 loss: 0.0447615422308445\n",
            "Epoch 491 loss: 0.04472564160823822\n",
            "Epoch 492 loss: 0.044689685106277466\n",
            "Epoch 493 loss: 0.04465359076857567\n",
            "Epoch 494 loss: 0.04461655020713806\n",
            "Epoch 495 loss: 0.044579438865184784\n",
            "Epoch 496 loss: 0.04454236850142479\n",
            "Epoch 497 loss: 0.04450524225831032\n",
            "Epoch 498 loss: 0.044468142092227936\n",
            "Epoch 499 loss: 0.044431887567043304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad():\n",
        "    plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "l-ltB7i4zQBe",
        "outputId": "df8741ce-b64c-4584-cb76-0725c695457f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad():\n",
        "    plt.plot(X, y)\n",
        "    plt.plot(X, neuron_2.forward(activation.forward(neuron_1.forward(X))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ySLH89TwzTeN",
        "outputId": "4dc5983a-6bdd-4763-fcd9-c834077b0b40"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-qUqdALiN-G"
      },
      "source": [
        "## 3.3 Построение сетей при помощи `torch.nn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Vxsck-1M6TAV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0ICJtarif3_"
      },
      "source": [
        "3.3.1 Решить задачу регрессии, соблюдая следующие условия:\n",
        "\n",
        "1. Оформить нейронную сеть в виде класса - наследника `nn.Module`\n",
        "2. При создании сети использовать готовые блоки из `torch.nn`: слои, функции активации, функции потерь и т.д.\n",
        "3. Для оптимизации использовать любой алгоритм оптимизации из `torch.optim` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "L1bvXHhO7aWs"
      },
      "outputs": [],
      "source": [
        "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
        "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SineNet(torch.nn.Module):\n",
        "    def __init__(self, n_hidden_neurons):\n",
        "        super(SineNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n",
        "        self.act1 = torch.nn.Sigmoid()\n",
        "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "neuron = SineNet(3)\n",
        "optimizer = optim.Adam(neuron.parameters(), lr=0.01)\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(2000):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = neuron.forward(X)\n",
        "    loss_val = loss(y_pred, y)\n",
        "\n",
        "    loss_val.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 100 == 0: print(f\"Epoch {epoch} loss: {loss_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3UyAlJs0mwy",
        "outputId": "44848bca-b082-48fe-824e-99fe1b5fcb0f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.5255182385444641\n",
            "Epoch 100 loss: 0.3011879026889801\n",
            "Epoch 200 loss: 0.19568783044815063\n",
            "Epoch 300 loss: 0.18882101774215698\n",
            "Epoch 400 loss: 0.18447914719581604\n",
            "Epoch 500 loss: 0.1798713058233261\n",
            "Epoch 600 loss: 0.17351046204566956\n",
            "Epoch 700 loss: 0.1635977029800415\n",
            "Epoch 800 loss: 0.1476832628250122\n",
            "Epoch 900 loss: 0.12290956825017929\n",
            "Epoch 1000 loss: 0.090749092400074\n",
            "Epoch 1100 loss: 0.057560764253139496\n",
            "Epoch 1200 loss: 0.03366099298000336\n",
            "Epoch 1300 loss: 0.020951665937900543\n",
            "Epoch 1400 loss: 0.014712627977132797\n",
            "Epoch 1500 loss: 0.011227095499634743\n",
            "Epoch 1600 loss: 0.00897987000644207\n",
            "Epoch 1700 loss: 0.007458603009581566\n",
            "Epoch 1800 loss: 0.00639308849349618\n",
            "Epoch 1900 loss: 0.005581664852797985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    plt.plot(X, y)\n",
        "    plt.plot(X, neuron(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2Shn9f9D0opV",
        "outputId": "bd190104-58c1-40d7-f6b1-d3d632138f18"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPUW6fm5jbQd"
      },
      "source": [
        "3.3.2 Решить задачу регрессии, соблюдая следующие условия:\n",
        "\n",
        "1. Оформить нейронную сеть в виде объекта `nn.Sequential`\n",
        "2. При создании сети использовать готовые блоки из `torch.nn`: слои, функции активации, функции потерь и т.д.\n",
        "3. Для оптимизации использовать любой алгоритм оптимизации из `torch.optim` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "BBwbAEd57a2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899e806b-c6c8-4e0c-9b3f-4ec2b961a1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.5020621418952942\n",
            "Epoch 100 loss: 0.21188442409038544\n",
            "Epoch 200 loss: 0.19594742357730865\n",
            "Epoch 300 loss: 0.19450236856937408\n",
            "Epoch 400 loss: 0.19226299226284027\n",
            "Epoch 500 loss: 0.18875256180763245\n",
            "Epoch 600 loss: 0.18317721784114838\n",
            "Epoch 700 loss: 0.1742786467075348\n",
            "Epoch 800 loss: 0.1582636535167694\n",
            "Epoch 900 loss: 0.12294939905405045\n",
            "Epoch 1000 loss: 0.06674294918775558\n",
            "Epoch 1100 loss: 0.025399066507816315\n",
            "Epoch 1200 loss: 0.009795728139579296\n",
            "Epoch 1300 loss: 0.006218850612640381\n",
            "Epoch 1400 loss: 0.004922184161841869\n",
            "Epoch 1500 loss: 0.003528864122927189\n",
            "Epoch 1600 loss: 0.002392119960859418\n",
            "Epoch 1700 loss: 0.0016014297725632787\n",
            "Epoch 1800 loss: 0.0011376144830137491\n",
            "Epoch 1900 loss: 0.0009100907482206821\n"
          ]
        }
      ],
      "source": [
        "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
        "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) \n",
        "\n",
        "layers = [\n",
        "    torch.nn.Linear(1, 5),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Linear(5, 1)\n",
        "]\n",
        "model = torch.nn.Sequential(*layers)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(2000):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model.forward(X)\n",
        "    loss_val = loss(y_pred, y)\n",
        "\n",
        "    loss_val.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 100 == 0: print(f\"Epoch {epoch} loss: {loss_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    plt.plot(X, y)\n",
        "    plt.plot(X, model(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "CAtoZODC0vwq",
        "outputId": "b6601cc7-f626-4856-e4e5-fcff35024d36"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQj0oVeLj2A1"
      },
      "source": [
        "## 3.4. Datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "c82tAkXMjajm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFPckkp8yhz"
      },
      "source": [
        "3.4.1 Создать датасет, поставляющий данные из задачи 3.1.2. \n",
        "\n",
        "Создать `DataLoader` на основе этого датасета и проверить работоспособность.\n",
        "\n",
        "Воспользовавшись результатами 3.3.1 (или 3.3.2) обучите модель, пользуясь мини-пакетным градиентным спуском с размером пакета (`batch_size`) = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tlcwQzCFRvFc"
      },
      "outputs": [],
      "source": [
        "class SinDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.X = torch.linspace(0, 1, 100).view(-1, 1)\n",
        "    self.y = torch.sin(2 * np.pi * self.X) + 0.1 * torch.rand(self.X.size()) \n",
        "    #pass\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "    #pass\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "    #pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxz02a3k_VQL"
      },
      "source": [
        "3.4.2 Предсказание цен алмазов\n",
        "\n",
        "3.4.2.1 Создайте датасет на основе файла diamonds.csv. \n",
        "\n",
        "1. Удалите все нечисловые столбцы\n",
        "2. Целевой столбец (`y`) - `price`\n",
        "3. Преобразуйте данные в тензоры корректных размеров\n",
        "\n",
        "3.4.2.2 Разбейте датасет на обучающий и тестовый датасет при помощи `torch.utils.data.random_split`.\n",
        "\n",
        "3.4.2.3 Обучите модель для предсказания цен при помощи мини-пакетного градиентного спуска (`batch_size = 256`). \n",
        "\n",
        "3.4.2.4 Выведите график функции потерь в зависимости от номера эпохи (значение потерь для эпохи рассчитывайте как среднее значение ошибок на каждом батче). Проверьте качество модели на тестовой выборке. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "fEfTNJQI8emD"
      },
      "outputs": [],
      "source": [
        "class DiamondsDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    super().__init__()\n",
        "    self.data = pd.read_csv(data, index_col = [0])\n",
        "    #pass\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    #pass\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    self.X = torch.FloatTensor(self.data.select_dtypes(['number']).values)\n",
        "    self.y = torch.FloatTensor(self.data['price'].values)\n",
        "    return self.X[idx], self.y[idx]\n",
        "    #pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DiamondsDataset('diamonds.csv')\n",
        "\n",
        "train, test = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.2)])\n",
        "\n",
        "diamonds_dataloader_train = DataLoader(train, batch_size=256)\n",
        "diamonds_dataloader_test = DataLoader(test, batch_size=256)"
      ],
      "metadata": {
        "id": "y_QWowvb11LH"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiamondsNet(torch.nn.Module):\n",
        "    def __init__(self, n_hidden_neurons, n_features):\n",
        "        super(DiamondsNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(n_hidden_neurons, n_features)\n",
        "        self.act1 = torch.nn.Sigmoid()\n",
        "        self.fc2 = torch.nn.Linear(n_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "neuron = DiamondsNet(7, 49)\n",
        "optimizer = torch.optim.SGD(neuron.parameters(), lr=0.025)\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "loss_vals = []\n",
        "for epoch in range(20):\n",
        "    X_new, y_new = next(iter(diamonds_dataloader_train))\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = neuron.forward(X_new)\n",
        "    loss_val = loss(y_pred, y_new)\n",
        "    loss_vals.append(loss_val)\n",
        "\n",
        "    loss_val.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch} loss: {loss_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-25n8BXI18Ee",
        "outputId": "2343272c-9ea9-4ba5-d117-b50582301465"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 33408504.0\n",
            "Epoch 1 loss: 17196130.0\n",
            "Epoch 2 loss: 17196130.0\n",
            "Epoch 3 loss: 17196130.0\n",
            "Epoch 4 loss: 17196130.0\n",
            "Epoch 5 loss: 17196130.0\n",
            "Epoch 6 loss: 17196130.0\n",
            "Epoch 7 loss: 17196130.0\n",
            "Epoch 8 loss: 17196130.0\n",
            "Epoch 9 loss: 17196130.0\n",
            "Epoch 10 loss: 17196130.0\n",
            "Epoch 11 loss: 17196130.0\n",
            "Epoch 12 loss: 17196130.0\n",
            "Epoch 13 loss: 17196130.0\n",
            "Epoch 14 loss: 17196130.0\n",
            "Epoch 15 loss: 17196130.0\n",
            "Epoch 16 loss: 17196130.0\n",
            "Epoch 17 loss: 17196130.0\n",
            "Epoch 18 loss: 17196130.0\n",
            "Epoch 19 loss: 17196130.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad():\n",
        "    plt.plot(loss_vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "ET8-BrIl2G96",
        "outputId": "14001732-5b1a-4970-bbf7-54aef798fda5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE81cgQdGM7I"
      },
      "source": [
        "3.4.3 Модифицируйте метод `__init__` датасета из 3.4.2 таким образом, чтобы он мог принимать параметр `transform: callable`. Реализуйте класс `DropColsTransform` для удаления нечисловых данных из массива. Реализуйте класс `ToTensorTransorm` для трансформации массива в тензор."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "J02LNj_F8qxK"
      },
      "outputs": [],
      "source": [
        "class DiamondsDataset(Dataset):\n",
        "  def __init__(self, data, transform=None):\n",
        "    self.data = pd.read_csv(data, index_col = [0])\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    #pass\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    X = self.data.drop('price', axis=1)\n",
        "    y = self.data['price']\n",
        "    sample = X.iloc[idx], y.iloc[idx]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "tJii-22pHIlU"
      },
      "outputs": [],
      "source": [
        "class DropColsTransform:\n",
        "  def __init__(self, drop):\n",
        "    self.drop = drop\n",
        "    #pass\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    X, y = sample\n",
        "    # <удаление из X столбцов self.drop>\n",
        "    X = X.drop(X.index[self.drop], axis=0)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "dZZ-OKrVHnY5"
      },
      "outputs": [],
      "source": [
        "class ToTensorTransform:\n",
        "  def __call__(self, sample):\n",
        "    X, y = sample\n",
        "    # <преобразование X и y в тензоры>\n",
        "    X = torch.FloatTensor(X.astype('float64').values)\n",
        "    y = torch.FloatTensor([y])\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "GssBjT9JHt5g"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "drop = DropColsTransform(drop=[1, 2, 3])\n",
        "to_tensor = ToTensorTransform()\n",
        "dataset = DiamondsDataset('diamonds.csv', transform=transforms.Compose([drop, to_tensor]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.2)])\n",
        "\n",
        "diamonds_dataloader_train = DataLoader(train, batch_size=256)\n",
        "diamonds_dataloader_test = DataLoader(test, batch_size=256)"
      ],
      "metadata": {
        "id": "Zxej3lGR27dO"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiamondsNet(torch.nn.Module):\n",
        "    def __init__(self, n_hidden_neurons, n_features):\n",
        "        super(DiamondsNet, self).__init__()\n",
        "        self.net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_features, n_hidden_neurons), \n",
        "            torch.nn.Sigmoid(), \n",
        "            torch.nn.Linear(n_hidden_neurons,n_hidden_neurons),\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(n_hidden_neurons,n_hidden_neurons*2),\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(n_hidden_neurons*2,1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "neuron = DiamondsNet(16, 6)\n",
        "optimizer = torch.optim.SGD(neuron.parameters(), lr=0.01)\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "loss_vals = []\n",
        "for epoch in range(35):\n",
        "    X_new, y_new = next(iter(diamonds_dataloader_train))\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = neuron.forward(X_new)\n",
        "    loss_val = loss(y_pred, y_new)\n",
        "    loss_vals.append(loss_val)\n",
        "\n",
        "    loss_val.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} loss: {loss_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlqSeyZy2-PW",
        "outputId": "303bf1aa-c82c-4abe-8072-e7f490f37981"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 27488948.0\n",
            "Epoch 1 loss: 22352088.0\n",
            "Epoch 2 loss: 16152399.0\n",
            "Epoch 3 loss: 14754689.0\n",
            "Epoch 4 loss: 14531059.0\n",
            "Epoch 5 loss: 14495279.0\n",
            "Epoch 6 loss: 14489555.0\n",
            "Epoch 7 loss: 14488638.0\n",
            "Epoch 8 loss: 14488491.0\n",
            "Epoch 9 loss: 14488468.0\n",
            "Epoch 10 loss: 14488465.0\n",
            "Epoch 11 loss: 14488464.0\n",
            "Epoch 12 loss: 14488464.0\n",
            "Epoch 13 loss: 14488465.0\n",
            "Epoch 14 loss: 14488464.0\n",
            "Epoch 15 loss: 14488465.0\n",
            "Epoch 16 loss: 14488465.0\n",
            "Epoch 17 loss: 14488463.0\n",
            "Epoch 18 loss: 14488463.0\n",
            "Epoch 19 loss: 14488463.0\n",
            "Epoch 20 loss: 14488463.0\n",
            "Epoch 21 loss: 14488463.0\n",
            "Epoch 22 loss: 14488463.0\n",
            "Epoch 23 loss: 14488463.0\n",
            "Epoch 24 loss: 14488463.0\n",
            "Epoch 25 loss: 14488463.0\n",
            "Epoch 26 loss: 14488463.0\n",
            "Epoch 27 loss: 14488463.0\n",
            "Epoch 28 loss: 14488463.0\n",
            "Epoch 29 loss: 14488463.0\n",
            "Epoch 30 loss: 14488463.0\n",
            "Epoch 31 loss: 14488463.0\n",
            "Epoch 32 loss: 14488463.0\n",
            "Epoch 33 loss: 14488463.0\n",
            "Epoch 34 loss: 14488463.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    plt.plot(loss_vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "SxiPp7zQ3EP2",
        "outputId": "bb6efa6c-5e43-4854-d3de-b37b7eb0993d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}